{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/29772158/make-ipython-notebook-print-in-real-time\n",
    "oldsysstdout = sys.stdout\n",
    "class flushfile():\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __getattr__(self,name): \n",
    "        return object.__getattribute__(self.f, name)\n",
    "    def write(self, x):\n",
    "        self.f.write(x)\n",
    "        self.f.flush()\n",
    "    def flush(self):\n",
    "        self.f.flush()\n",
    "sys.stdout = flushfile(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enumerate Images\n",
    "Image names are sequential, so add every tenth image to the validation set based on filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = []\n",
    "test = []\n",
    "\n",
    "for root, dirs, files in os.walk('captures'):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        low_name = name.lower()\n",
    "        # Find all the image files, split into test and training.\n",
    "        if low_name.endswith(\".png\"):\n",
    "            if low_name.endswith(\"0.png\"):\n",
    "                test.append(path)\n",
    "            else:\n",
    "                training.append(path)\n",
    "\n",
    "print(\"Training:\", len(training), \"Test:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "Each image file contains a color image (top half), and an encoded depth image (bottom half)\n",
    "<img src=\"testing/IMG_2114.PNG\">\n",
    "* Note: The image may also contain the orientation data. If so it is encoded in the first two pixels of the depth image. If the first pixel is red, the second has the x, y, z, w quaternion components encoded in the r,g,b,a values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(image):\n",
    "    \"\"\"Split the image data into the top and bottom half.\"\"\"\n",
    "    split_height = image.shape[0] / 2\n",
    "    return image[:split_height], image[split_height:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_depth(image):\n",
    "    \"\"\"12 bits of depth in millimeters is encoded with 6 bits in red and 3 bits in each of green and blue.\"\"\"\n",
    "    BYTE_MAX = 255\n",
    "    CHANNEL_MAX = 8.0\n",
    "    MAX_RED_VALUE = BYTE_MAX - CHANNEL_MAX\n",
    "    CHANNELS_MAX = CHANNEL_MAX * CHANNEL_MAX\n",
    "    orientation = [1, 0, 0, 0] # default orientation if not present in image.\n",
    "    \n",
    "    if np.array_equal(image[0, 0], [BYTE_MAX, 0, 0, BYTE_MAX]):\n",
    "        # Orientation quaternion is present.\n",
    "        pixel = image[0, 1]\n",
    "        for c in range(len(orientation)):\n",
    "            orientation[c] = ((2.0 * pixel[c]) / BYTE_MAX) - 1\n",
    "\n",
    "        # Clear out the pixels so they don't get interepreted as depth.\n",
    "        image[0, 0] = [0, 0, 0, BYTE_MAX]\n",
    "        image[0, 1] = [0, 0, 0, BYTE_MAX]\n",
    "\n",
    "    red = image[:, :, 0]\n",
    "    green = image[:, :, 1]\n",
    "    blue = image[:, :, 2]\n",
    "\n",
    "    depth = ((MAX_RED_VALUE - red) * CHANNELS_MAX) + ((green - red) * CHANNEL_MAX) + (blue - red)\n",
    "    \n",
    "    # Zero in the red channel indicates the sensor provided no data.\n",
    "    depth[np.where(red == 0)] = float('nan')\n",
    "    return depth, orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NaNs with localized stat values using mipmaps\n",
    "Combined this: http://stackoverflow.com/questions/14549696/mipmap-of-image-in-numpy\n",
    "\n",
    "With this: http://stackoverflow.com/questions/5480694/numpy-calculate-averages-with-nans-removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mipmap_imputer(image, strategy=np.mean, scales=None):\n",
    "    scales = scales if scales else [(5,5), (3,2), (2,2), (2,2), (2,2), (2,2), (2,2), (1,2)]\n",
    "    mipmaps = []\n",
    "    mipmap = image\n",
    "    for y, x in scales:\n",
    "        mipmap = mipmap.copy()\n",
    "        size = mipmap.shape\n",
    "        reshaped = mipmap.reshape(size[0] / y, y, size[1] / x, x)\n",
    "        masked = np.ma.masked_array(reshaped, np.isnan(reshaped))\n",
    "        mipmap = strategy(strategy(masked, axis=3), axis=1).filled(np.nan)\n",
    "        mipmaps.append(mipmap)\n",
    "    \n",
    "    for index, mipmap in reversed(list(enumerate(mipmaps))):\n",
    "        y, x = scales[index]\n",
    "        expanded = mipmap\n",
    "        if x > 1:\n",
    "            expanded = np.repeat(expanded, x, axis=1).reshape(expanded.shape[0], expanded.shape[1] * x)\n",
    "        if y > 1:\n",
    "            expanded = np.repeat(expanded, y, axis=0).reshape(expanded.shape[0] * y, expanded.shape[1])\n",
    "        target = mipmaps[index - 1] if index > 0 else image.copy()\n",
    "\n",
    "        nans = np.where(np.isnan(target))\n",
    "        target[nans] = expanded[nans]\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Node/Layer Types:\n",
    "* Matrix\n",
    " * Dimensions (height, width, depth)\n",
    "* Relu\n",
    "* Dropout\n",
    " * Fraction\n",
    "* Conv\n",
    " * Dimensions (height, width, channels)\n",
    " * Stride (height, width)\n",
    " * Padding Type (same, valid)\n",
    "* Pool\n",
    " * Type (max, avg)\n",
    " * Size (height, width)\n",
    " * Stride (height, width) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = '../ud730/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"image_size\": 28,\n",
    "    \"label_count\": 10,\n",
    "    \"channel_count\": 1\n",
    "}\n",
    "datasets[\"total_image_size\"] = datasets[\"image_size\"] * datasets[\"image_size\"]\n",
    "\n",
    "def reformat(dataset, labels, name):\n",
    "    dataset = dataset.reshape((-1, datasets[\"image_size\"], datasets[\"image_size\"], datasets[\"channel_count\"])).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(datasets[\"label_count\"]) == labels[:,None]).astype(np.float32)\n",
    "    print(name + \" set\", dataset.shape, labels.shape)\n",
    "    return dataset, labels\n",
    "datasets[\"train\"], datasets[\"train_labels\"] = reformat(train_dataset, train_labels, \"Training\")\n",
    "datasets[\"valid\"], datasets[\"valid_labels\"] = reformat(valid_dataset, valid_labels, \"Validation\")\n",
    "datasets[\"test\"], datasets[\"test_labels\"] = reformat(test_dataset, test_labels, \"Test\")\n",
    "\n",
    "print(datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(graph_info, data, step_count, report_every=50):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        for step in xrange(step_count + 1):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (data[\"train_labels\"].shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = data[\"train\"][offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = data[\"train_labels\"][offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            targets = [graph_info[\"optimizer\"], graph_info[\"loss\"], graph_info[\"predictions\"]]\n",
    "            feed_dict = {graph_info[\"train\"] : batch_data, graph_info[\"labels\"] : batch_labels}\n",
    "            _, l, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "            if (step % report_every == 0):\n",
    "                print(\"Minibatch loss at step\", step, \":\", l)\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(graph_info[\"valid\"].eval(), data[\"valid_labels\"]))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(graph_info[\"test\"].eval(), data[\"test_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter setup functions\n",
    "\n",
    "def no_parameters(options):\n",
    "    return ()\n",
    "\n",
    "def setup_matrix(options):\n",
    "    initialize_matrix = options[\"init\"]\n",
    "    size = options[\"size\"]\n",
    "    matrix = tf.Variable(initialize_matrix(size))\n",
    "    if options[\"bias\"]:\n",
    "        initialize_bias = options[\"bias_init\"]\n",
    "        bias = tf.Variable(initialize_bias(size[-1:]))\n",
    "        return (matrix, bias)\n",
    "    return (matrix,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DISTRIBUTIONS = [\n",
    "    \"constant\",\n",
    "    \"uniform\",\n",
    "    \"normal\",\n",
    "    \"truncated\"\n",
    "]\n",
    "\n",
    "def setup_initializer(mean=0.0, scale=1.0, distribution=\"constant\", seed=None):\n",
    "    return {\n",
    "        DISTRIBUTIONS[0]: lambda shape: tf.fill(shape, mean),\n",
    "        DISTRIBUTIONS[1]: lambda shape: tf.random_uniform(shape, mean - scale, mean + scale, seed=seed),\n",
    "        DISTRIBUTIONS[2]: lambda shape: tf.random_normal(shape, mean, scale, seed=seed),\n",
    "        DISTRIBUTIONS[3]: lambda shape: tf.truncated_normal(shape, mean, scale, seed=seed)\n",
    "    }[distribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def same_output_shape(input_shape, options):\n",
    "    return input_shape\n",
    "\n",
    "def matrix_output_shape(input_shape, options):\n",
    "    matrix_size = options[\"size\"]\n",
    "    return (int(input_shape[0]), matrix_size[1])\n",
    "\n",
    "def flatten_output_shape(input_shape, options):\n",
    "    return (int(input_shape[0]), int(input_shape[1] * input_shape[2] * input_shape[3]))\n",
    "\n",
    "def unflatten_output_shape(input_shape, options):\n",
    "    size = options[\"size\"]\n",
    "    pixels = size[0] * size[1]\n",
    "    return (int(input_shape[0]), size[0], size[1], int(input_shape[1] / pixels))\n",
    "\n",
    "def image_output_shape(input_shape, size, stride, padding):\n",
    "    if len(size) > 2 and input_shape[3] != size[2]:\n",
    "        print(\"Matrix size incompatible!\")\n",
    "\n",
    "    height = size[0]\n",
    "    width  = size[1]\n",
    "    out_depth = size[3] if len(size) > 2 else int(input_shape[3])\n",
    "    \n",
    "    input_height = input_shape[1]\n",
    "    input_width  = input_shape[2]\n",
    "    \n",
    "    if padding == \"VALID\":\n",
    "        input_height -= height - 1\n",
    "        input_width  -= width - 1\n",
    "    \n",
    "    return (\n",
    "        int(input_shape[0]),\n",
    "        (input_height + stride[0] - 1) / stride[0],\n",
    "        (input_width  + stride[1] - 1) / stride[1],\n",
    "        out_depth\n",
    "    )\n",
    "\n",
    "def image_size_options(options):\n",
    "    return {key: options[key] for key in ('size', 'stride', 'padding')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Node connection functions\n",
    "\n",
    "def apply_matrix(input_node, train, parameters, options):\n",
    "    application = tf.matmul(input_node, parameters[0])\n",
    "    if len(parameters) > 1:\n",
    "        return application + parameters[1]\n",
    "    return application\n",
    "\n",
    "def apply_relu(input_node, train, parameters, options):\n",
    "    return tf.nn.relu(input_node)\n",
    "\n",
    "def apply_dropout(input_node, train, parameters, options):\n",
    "    if train:\n",
    "        return tf.nn.dropout(input_node, options[\"dropout_rate\"], seed=options[\"seed\"])\n",
    "    else:\n",
    "        return input_node\n",
    "\n",
    "def apply_conv(input_node, train, parameters, options):\n",
    "    stride = options[\"stride\"]\n",
    "    output = tf.nn.conv2d(input_node, parameters[0], [1, stride[0], stride[1], 1], padding=options[\"padding\"])\n",
    "    \n",
    "    if options[\"bias\"]:\n",
    "        output = output + parameters[1]\n",
    "        \n",
    "    return output\n",
    "\n",
    "def apply_pool(input_node, train, parameters, options):\n",
    "    if options[\"pool_type\"].startswith(\"max\"):\n",
    "        pool_function = tf.nn.max_pool\n",
    "    else:\n",
    "        pool_function = tf.nn.avg_pool\n",
    "    stride = [1, options[\"stride\"][0], options[\"stride\"][1], 1]\n",
    "    size = [1, options[\"size\"][0], options[\"size\"][1], 1]\n",
    "    return pool_function(input_node, size, stride, padding=options[\"padding\"])\n",
    "\n",
    "def apply_flatten(input_node, train, parameters, options):\n",
    "    return tf.reshape(input_node, flatten_output_shape(input_node.get_shape(), options))\n",
    "\n",
    "def apply_unflatten(input_node, train, parameters, options):\n",
    "    return tf.reshape(input_node, unflatten_output_shape(input_node.get_shape(), options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shape_test(shape, options, func):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        input = tf.placeholder(tf.float32, shape=shape)\n",
    "        parameters = setup_matrix(options)\n",
    "        result = func(input, False, parameters, options)\n",
    "        return tuple(int(d) for d in result.get_shape())\n",
    "    \n",
    "default_init = setup_initializer()\n",
    "correct = 0\n",
    "for w in xrange(1, 7):\n",
    "    for p in xrange(1, w + 1):\n",
    "        for s in xrange(1, p + 1):\n",
    "            for pad in [\"SAME\", \"VALID\"]:\n",
    "                for func in [apply_pool, apply_conv]:\n",
    "                    options = {\n",
    "                        \"size\":(p, p, 1, 1),\n",
    "                        \"stride\": (s, s),\n",
    "                        \"padding\":pad,\n",
    "                        \"pool_type\": \"max\",\n",
    "                        \"bias\":False,\n",
    "                        \"init\":default_init}\n",
    "                    calc = image_output_shape([1, w, w, 1], **image_size_options(options))\n",
    "                    shape = shape_test([1, w, w, 1], options, func)\n",
    "                    if calc == shape:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        print(\"mismatch for\", w, p, s, pad, shape, calc)\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Setup and keep track of graph parameters and nodes for a layer.\"\"\"\n",
    "    def __init__(self, options, parameter_setup, node_setup):\n",
    "        self.options = options\n",
    "        self.parameter_setup = parameter_setup\n",
    "        self.node_setup = node_setup\n",
    "        self.parameters = None\n",
    "        \n",
    "    def setup_parameters(self):\n",
    "        self.parameters = self.parameter_setup(self.options)\n",
    "        \n",
    "    def connect(self, input_node, train):\n",
    "        node = self.node_setup(input_node, train, self.parameters, self.options)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Layer setup functions.\n",
    "\n",
    "def create_matrix_layer(inputs, outputs, bias=True, init=setup_initializer(distribution=\"normal\", scale=0.1)):\n",
    "    size = (inputs, outputs)\n",
    "    options = {\n",
    "        \"size\": size,\n",
    "        \"bias\": bias,\n",
    "        \"init\": lambda size: init(size),\n",
    "        \"bias_init\": lambda size: init((outputs,))\n",
    "    }\n",
    "    return Layer(options, setup_matrix, apply_matrix)\n",
    "\n",
    "def create_relu_layer():\n",
    "    return Layer({}, no_parameters, apply_relu)\n",
    "\n",
    "def create_dropout_layer(rate, seed):\n",
    "    options = {\n",
    "        \"dropout_rate\": rate,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "    return Layer(options, no_parameters, apply_dropout)\n",
    "\n",
    "def create_conv_layer(patch_size, stride, in_channels, out_channels, bias=True, padding=\"SAME\"):\n",
    "    init = lambda size: tf.truncated_normal(size, stddev=0.1)\n",
    "    options = {\n",
    "        \"size\": patch_size + (in_channels, out_channels),\n",
    "        \"bias\": bias,\n",
    "        \"init\": init,\n",
    "        \"bias_init\": init,\n",
    "        \"stride\": stride,\n",
    "        \"padding\": padding\n",
    "    }\n",
    "    return Layer(options, setup_matrix, apply_conv)\n",
    "\n",
    "def create_pool_layer(strategy, patch_size, stride, padding=\"SAME\"):\n",
    "    options = {\n",
    "        \"pool_type\": strategy,\n",
    "        \"size\": patch_size,\n",
    "        \"stride\": stride,\n",
    "        \"padding\": padding\n",
    "    }\n",
    "    return Layer(options, no_parameters, apply_pool)\n",
    "\n",
    "def create_flatten_layer():\n",
    "    options = {}\n",
    "    return Layer(options, no_parameters, apply_flatten)\n",
    "\n",
    "def create_unflatten_layer(size):\n",
    "    options = {\n",
    "        \"size\": size\n",
    "    }\n",
    "    return Layer(options, no_parameters, apply_unflatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Mutagen(object):\n",
    "    def __init__(self, seed):\n",
    "        self.toggle_relu = 0.01\n",
    "        self.change_dropout_rate = 0.05\n",
    "        self.DROPOUT_GRANULARITY = 4\n",
    "        self.output_size_factors = [\n",
    "            (0.02, 0.5),\n",
    "            (0.01, 0.75),\n",
    "            (0.01, 0.9),\n",
    "            (0.02, 1.1),\n",
    "            (0.02, 1.25),\n",
    "            (0.02, 2.0)\n",
    "        ]\n",
    "        self.change_distribution = 0.01\n",
    "        self.initial_means = [\n",
    "            (0.03, 0)\n",
    "            (0.02, 1)\n",
    "            (0.01, -1)\n",
    "        ]\n",
    "        self.image_operations = [\n",
    "            (0.05, \"conv_bias\"),\n",
    "            (0.01, \"conv\"),\n",
    "            (0.03, \"max_pool\"),\n",
    "            (0.03, \"avg_pool\")\n",
    "        ]\n",
    "        self.patches = [\n",
    "            (0.005, 1),\n",
    "            (0.008, 2),\n",
    "            (0.009, 3),\n",
    "            (0.010, 4),\n",
    "            (0.015, 5),\n",
    "            (0.010, 6),\n",
    "            (0.009, 7),\n",
    "            (0.008, 8),\n",
    "            (0.007, 9),\n",
    "            (0.006, 10),\n",
    "            (0.005, 11),\n",
    "            (0.004, 12),\n",
    "            (0.003, 13),\n",
    "            (0.002, 14),\n",
    "            (0.001, 15)\n",
    "        ]\n",
    "        self.strides = [\n",
    "            (0.005, 1),\n",
    "            (0.010, 2),\n",
    "            (0.002, 3),\n",
    "            (0.001, 4),\n",
    "            (0.001, 5)\n",
    "        ]\n",
    "        self.paddings = [\n",
    "            (0.05, \"SAME\"),\n",
    "            (0.05, \"VALID\")\n",
    "        ]\n",
    "        self.entropy = random.Random(seed)\n",
    "        \n",
    "    def branch(self, bias):\n",
    "        return self.entropy.random() < bias\n",
    "    \n",
    "    def select(self, choices, default=None):\n",
    "        value = self.entropy.random()\n",
    "        threshold = 0\n",
    "        for entry in choices:\n",
    "            threshold += entry[0]\n",
    "            if value < threshold:\n",
    "                return entry[1]\n",
    "        return default\n",
    "        \n",
    "    def mutate_relu(self, relu):\n",
    "        return (not relu) if self.branch(self.toggle_relu) else relu\n",
    "    \n",
    "    def mutate_dropout(self, rate):\n",
    "        if self.branch(self.change_dropout_rate):\n",
    "            return (1.0 / self.DROPOUT_GRANULARITY) * self.entropy.randint(0, self.DROPOUT_GRANULARITY - 1)\n",
    "        return rate\n",
    "    \n",
    "    def mutate_output_size(self, output_size):\n",
    "        return int(output_size * self.select(self.output_size_factors, 1))\n",
    "    \n",
    "    def mutate_distribution(self, distribution):\n",
    "        if self.branch(self.change_distribution):\n",
    "            return self.entropy.choice(DISTRIBUTIONS)\n",
    "        return distribution\n",
    "    \n",
    "    def mutate_initial_mean(self, mean):\n",
    "        return self.select(self.initial_means, mean)\n",
    "    \n",
    "    def mutate_initial_scale(self, scale):\n",
    "        return scale * self.select(self.output_size_factors, 1)\n",
    "    \n",
    "    def mutate_image_operation(self, operation):\n",
    "        return self.select(self.image_operations, operation)\n",
    "    \n",
    "    def mutate_patch_size(self, patch_size):\n",
    "        return self.select(self.patches, patch_size)\n",
    "    \n",
    "    def mutate_stride(self, stride):\n",
    "        return self.select(self.strides, stride)\n",
    "    \n",
    "    def mutate_padding(self, padding):\n",
    "        return self.select(self.paddings, padding)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvoLayer(object):\n",
    "    \"\"\"An evolvable layer representation.\n",
    "    Each layer consists of either a conv or pool in the image section, or a hidden layer otherwise,\n",
    "    followed optionally by a relu and/or dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, primary):\n",
    "        self.primary = primary\n",
    "        self.dropout_rate = 0\n",
    "        self.dropout_seed = None # Use graph level seed.\n",
    "        self.relu = False\n",
    "        \n",
    "    def output_size(self, input_size):\n",
    "        return self.primary.output_size(input_size)\n",
    "    \n",
    "    def reseed(self, entropy):\n",
    "        self.primary.reseed(entropy)\n",
    "        self.dropout_seed = entropy.randint(1, 100000)\n",
    "    \n",
    "    def mutate(self, mutagen):\n",
    "        self.primary.mutate(mutagen)\n",
    "        self.relu = mutagen.mutate_relu(self.relu)\n",
    "        self.dropout_rate = mutagen.mutate_dropout(self.dropout_rate)\n",
    "    \n",
    "    def construct(self, input_size, layers):\n",
    "        layers.append(self.primary.construct(input_size))\n",
    "        if self.relu:\n",
    "            layers.append(create_relu_layer())\n",
    "        if self.dropout_rate > 0:\n",
    "            layers.append(create_dropout_layer(self.dropout_rate, self.dropout_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Initializer(object):\n",
    "    def __init__(self, distribution=\"constant\", mean=0.0, scale=1.0):\n",
    "        self.distribution = distribution\n",
    "        self.mean = mean\n",
    "        self.scale = scale\n",
    "        self.seed = None\n",
    "        \n",
    "    def mutate(self, mutagen):\n",
    "        self.distribution = mutagen.mutate_distribution(self.distribution)\n",
    "        self.mean = mutagen.mutate_initial_mean(self.mean)\n",
    "        self.scale = mutagen.mutate_initial_scale(self.scale)\n",
    "        \n",
    "    def reseed(self, entropy):\n",
    "        self.seed = entropy.randint(1, 100000)\n",
    "        \n",
    "    def construct(self):\n",
    "        return setup_initializer(self.mean, self.scale, self.distribution, self.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, output_size, bias, initializer):\n",
    "        self.output_size = output_size\n",
    "        self.bias = bias\n",
    "        self.initializer = initializer\n",
    "        \n",
    "    def output_size(self, input_size):\n",
    "        return self.output_size\n",
    "        \n",
    "    def mutate(self, mutagen):\n",
    "        self.output_size = mutagen.mutate_output_size(self.output_size)\n",
    "        self.initializer.mutate(mutagen)\n",
    "        \n",
    "    def reseed(self, entropy):\n",
    "        self.initializer.reseed(entropy)\n",
    "        \n",
    "    def construct(self, input_size):\n",
    "        return create_matrix_layer(input_size, output_size, self.bias, self.initializer.construct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageLayer(object):\n",
    "    def __init__(self, operation, patch_size, stride, output_channels, padding, initializer):\n",
    "        self.operation = operation\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.output_channels\n",
    "        self.padding = padding\n",
    "        self.initializer = initializer\n",
    "        \n",
    "    def output_size(self, input_size):\n",
    "        return self.output_size\n",
    "        \n",
    "    def mutate(self, mutagen):\n",
    "        self.operation = mutagen.mutate_image_operation(self.operation)\n",
    "        self.patch_size = mutagen.mutate_patch_size(self.patch_size)\n",
    "        self.stride = mutagen.mutate_stride(self.stride)\n",
    "        self.padding = mutagen.mutate_padding(self.padding)\n",
    "        self.output_channels = mutagen.mutate_output_size(self.output_channels)\n",
    "        self.initializer.mutate(mutagen)\n",
    "        \n",
    "    def reseed(self, entropy):\n",
    "        self.initializer.reseed(entropy)\n",
    "        \n",
    "    def construct(self, input_size):\n",
    "        if self.operation.startswith(\"conv\"):\n",
    "            return create_conv_layer(\n",
    "                (self.patch_size, self.patch_size),\n",
    "                (self.stride, self.stride),\n",
    "                input_size[3], self.output_channels,\n",
    "                self.operation.endswith(\"bias\"), self.padding\n",
    "            )\n",
    "        else:\n",
    "            return create_pool_layer(\n",
    "                self.operation,\n",
    "                (self.patch_size, self.patch_size)\n",
    "                (self.stride, self.stride),\n",
    "                self.padding\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerStack(object):\n",
    "    \"\"\"Overall structure for the network\"\"\"\n",
    "    def __init__(self, input_size, flatten, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.flatten = flatten\n",
    "        self.output_size = output_size\n",
    "        self.layers = []\n",
    "        \n",
    "    def mutate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_two_layer(batch_size, patch_size, depth, hidden_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        train = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, channel_count))\n",
    "        labels= tf.placeholder(tf.float32, shape=(batch_size, label_count))\n",
    "        valid = tf.constant(data[\"valid\"])\n",
    "        test  = tf.constant(data[\"test\"])\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        layers = [\n",
    "            create_conv_layer((patch_size, patch_size), (stride, stride), 1, depth),\n",
    "            create_relu_layer(),\n",
    "            create_conv_layer((patch_size, patch_size), (stride, stride), depth, depth),\n",
    "            create_relu_layer(),\n",
    "            create_flatten_layer(),\n",
    "            create_matrix_layer(image_size * image_size * depth / pow(stride, 4), hidden_size),\n",
    "            create_relu_layer(),\n",
    "            create_matrix_layer(hidden_size, label_count)\n",
    "        ]\n",
    "        \n",
    "        for layer in layers:\n",
    "            layer.setup_parameters()\n",
    "        \n",
    "        def model(nodes, train):\n",
    "            for layer in layers:\n",
    "                nodes.append(layer.connect(nodes[-1], train))\n",
    "            return nodes[-1]\n",
    "\n",
    "        logits = model([train], True)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(0.05).minimize(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"valid\": tf.nn.softmax(model([valid], False)),\n",
    "            \"test\":  tf.nn.softmax(model([test], False))\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_2conv = convnet_two_layer(batch_size=16, patch_size=5, depth=16, hidden_size=64, data=datasets)\n",
    "\n",
    "run_graph(graph_2conv, datasets, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_steps = 200000\n",
    "\n",
    "graph_connive = convnet_optimize(\n",
    "    batch_size=16, patch_sizes=[5,14], strides=[2,7], depths=[16,128],\n",
    "    hidden_sizes=[128,64],\n",
    "    rate_alpha=0.02, decay_rate=0.9, decay_steps=optimal_steps/4,\n",
    "    beta_loss=0.0005,\n",
    "    dropout_rate=0.5,\n",
    "    base_seed=45645,\n",
    "    data=datasets)\n",
    "\n",
    "run_graph(graph_connive, datasets, optimal_steps, report_every=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
