{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/29772158/make-ipython-notebook-print-in-real-time\n",
    "oldsysstdout = sys.stdout\n",
    "class flushfile():\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __getattr__(self,name): \n",
    "        return object.__getattribute__(self.f, name)\n",
    "    def write(self, x):\n",
    "        self.f.write(x)\n",
    "        self.f.flush()\n",
    "    def flush(self):\n",
    "        self.f.flush()\n",
    "sys.stdout = flushfile(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enumerate Images\n",
    "Image names are sequential, so add every tenth image to the validation set based on filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = []\n",
    "test = []\n",
    "\n",
    "for root, dirs, files in os.walk('captures'):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        low_name = name.lower()\n",
    "        # Find all the image files, split into test and training.\n",
    "        if low_name.endswith(\".png\"):\n",
    "            if low_name.endswith(\"0.png\"):\n",
    "                test.append(path)\n",
    "            else:\n",
    "                training.append(path)\n",
    "\n",
    "print(\"Training:\", len(training), \"Test:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "Each image file contains a color image (top half), and an encoded depth image (bottom half)\n",
    "<img src=\"testing/IMG_2114.PNG\">\n",
    "* Note: The image may also contain the orientation data. If so it is encoded in the first two pixels of the depth image. If the first pixel is red, the second has the x, y, z, w quaternion components encoded in the r,g,b,a values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(image):\n",
    "    \"\"\"Split the image data into the top and bottom half.\"\"\"\n",
    "    split_height = image.shape[0] / 2\n",
    "    return image[:split_height], image[split_height:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_depth(image):\n",
    "    \"\"\"12 bits of depth in millimeters is encoded with 6 bits in red and 3 bits in each of green and blue.\"\"\"\n",
    "    BYTE_MAX = 255\n",
    "    CHANNEL_MAX = 8.0\n",
    "    MAX_RED_VALUE = BYTE_MAX - CHANNEL_MAX\n",
    "    CHANNELS_MAX = CHANNEL_MAX * CHANNEL_MAX\n",
    "    orientation = [1, 0, 0, 0] # default orientation if not present in image.\n",
    "    \n",
    "    if np.array_equal(image[0, 0], [BYTE_MAX, 0, 0, BYTE_MAX]):\n",
    "        # Orientation quaternion is present.\n",
    "        pixel = image[0, 1]\n",
    "        for c in range(len(orientation)):\n",
    "            orientation[c] = ((2.0 * pixel[c]) / BYTE_MAX) - 1\n",
    "\n",
    "        # Clear out the pixels so they don't get interepreted as depth.\n",
    "        image[0, 0] = [0, 0, 0, BYTE_MAX]\n",
    "        image[0, 1] = [0, 0, 0, BYTE_MAX]\n",
    "\n",
    "    red = image[:, :, 0]\n",
    "    green = image[:, :, 1]\n",
    "    blue = image[:, :, 2]\n",
    "\n",
    "    depth = ((MAX_RED_VALUE - red) * CHANNELS_MAX) + ((green - red) * CHANNEL_MAX) + (blue - red)\n",
    "    \n",
    "    # Zero in the red channel indicates the sensor provided no data.\n",
    "    depth[np.where(red == 0)] = float('nan')\n",
    "    return depth, orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NaNs with localized stat values using mipmaps\n",
    "Combined this: http://stackoverflow.com/questions/14549696/mipmap-of-image-in-numpy\n",
    "\n",
    "With this: http://stackoverflow.com/questions/5480694/numpy-calculate-averages-with-nans-removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mipmap_imputer(image, strategy=np.mean, scales=None):\n",
    "    scales = scales if scales else [(5,5), (3,2), (2,2), (2,2), (2,2), (2,2), (2,2), (1,2)]\n",
    "    mipmaps = []\n",
    "    mipmap = image\n",
    "    for y, x in scales:\n",
    "        mipmap = mipmap.copy()\n",
    "        size = mipmap.shape\n",
    "        reshaped = mipmap.reshape(size[0] / y, y, size[1] / x, x)\n",
    "        masked = np.ma.masked_array(reshaped, np.isnan(reshaped))\n",
    "        mipmap = strategy(strategy(masked, axis=3), axis=1).filled(np.nan)\n",
    "        mipmaps.append(mipmap)\n",
    "    \n",
    "    for index, mipmap in reversed(list(enumerate(mipmaps))):\n",
    "        y, x = scales[index]\n",
    "        expanded = mipmap\n",
    "        if x > 1:\n",
    "            expanded = np.repeat(expanded, x, axis=1).reshape(expanded.shape[0], expanded.shape[1] * x)\n",
    "        if y > 1:\n",
    "            expanded = np.repeat(expanded, y, axis=0).reshape(expanded.shape[0] * y, expanded.shape[1])\n",
    "        target = mipmaps[index - 1] if index > 0 else image.copy()\n",
    "\n",
    "        nans = np.where(np.isnan(target))\n",
    "        target[nans] = expanded[nans]\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Node/Layer Types:\n",
    "* Matrix\n",
    " * Dimensions (height, width, depth)\n",
    "* Relu\n",
    "* Dropout\n",
    " * Fraction\n",
    "* Conv\n",
    " * Dimensions (height, width, channels)\n",
    " * Stride (height, width)\n",
    " * Padding Type (same, valid)\n",
    "* Pool\n",
    " * Type (max, avg)\n",
    " * Size (height, width)\n",
    " * Stride (height, width) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = '../ud730/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"image_size\": 28,\n",
    "    \"label_count\": 10,\n",
    "    \"channel_count\": 1\n",
    "}\n",
    "datasets[\"total_image_size\"] = datasets[\"image_size\"] * datasets[\"image_size\"]\n",
    "\n",
    "def reformat(dataset, labels, name):\n",
    "    dataset = dataset.reshape((-1, datasets[\"image_size\"], datasets[\"image_size\"], datasets[\"channel_count\"])).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(datasets[\"label_count\"]) == labels[:,None]).astype(np.float32)\n",
    "    print(name + \" set\", dataset.shape, labels.shape)\n",
    "    return dataset, labels\n",
    "datasets[\"train\"], datasets[\"train_labels\"] = reformat(train_dataset, train_labels, \"Training\")\n",
    "datasets[\"valid\"], datasets[\"valid_labels\"] = reformat(valid_dataset, valid_labels, \"Validation\")\n",
    "datasets[\"test\"], datasets[\"test_labels\"] = reformat(test_dataset, test_labels, \"Test\")\n",
    "\n",
    "print(datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(graph_info, data, step_count, report_every=50):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        for step in xrange(step_count + 1):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (data[\"train_labels\"].shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = data[\"train\"][offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = data[\"train_labels\"][offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            targets = [graph_info[\"optimizer\"], graph_info[\"loss\"], graph_info[\"predictions\"]]\n",
    "            feed_dict = {graph_info[\"train\"] : batch_data, graph_info[\"labels\"] : batch_labels}\n",
    "            _, l, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "            if (step % report_every == 0):\n",
    "                print(\"Minibatch loss at step\", step, \":\", l)\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(graph_info[\"valid\"].eval(), data[\"valid_labels\"]))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(graph_info[\"test\"].eval(), data[\"test_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter setup functions\n",
    "\n",
    "def no_parameters(options):\n",
    "    return ()\n",
    "\n",
    "def setup_matrix(options):\n",
    "    initialize_matrix = options[\"init\"]\n",
    "    size = options[\"size\"]\n",
    "    matrix = tf.Variable(initialize_matrix(size))\n",
    "    if options[\"bias\"]:\n",
    "        initialize_bias = options[\"bias_init\"]\n",
    "        bias = tf.Variable(initialize_bias(size[-1:]))\n",
    "        return (matrix, bias)\n",
    "    return (matrix,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def same_output_shape(input_shape, options):\n",
    "    return input_shape\n",
    "\n",
    "def matrix_output_shape(input_shape, options):\n",
    "    matrix_size = options[\"size\"]\n",
    "    return (int(input_shape[0]), matrix_size[1])\n",
    "\n",
    "def flatten_output_shape(input_shape, options):\n",
    "    return (int(input_shape[0]), int(input_shape[1] * input_shape[2] * input_shape[3]))\n",
    "\n",
    "def unflatten_output_shape(input_shape, options):\n",
    "    size = options[\"size\"]\n",
    "    pixels = size[0] * size[1]\n",
    "    return (int(input_shape[0]), size[0], size[1], int(input_shape[1] / pixels))\n",
    "\n",
    "def conv_output_shape(input_shape, options):\n",
    "    size = options[\"size\"]\n",
    "    same_padding = options[\"padding\"] == \"SAME\"\n",
    "    stride = options[\"stride\"]\n",
    "    \n",
    "    if len(size) > 2 and input_shape[3] != size[2]:\n",
    "        print(\"Matrix size incompatible!\")\n",
    "\n",
    "    height = size[0]\n",
    "    width = size[1]\n",
    "    out_depth = size[3] if len(size) > 2 else int(input_shape[3])\n",
    "    \n",
    "    input_height = input_shape[1]\n",
    "    input_width = input_shape[2]\n",
    "    \n",
    "    if not same_padding:\n",
    "        input_height -= height\n",
    "        input_width -= width\n",
    "    \n",
    "    return (\n",
    "        int(input_shape[0]),\n",
    "        (input_height + stride[0] - 1) / stride[0],\n",
    "        (input_width + stride[1] - 1)/ stride[1],\n",
    "        out_depth\n",
    "    )\n",
    "\n",
    "print(conv_output_shape([1, 25, 40, 1], {\"size\":(4, 5), \"stride\": (2, 2), \"padding\":\"VALID\"}))\n",
    "print(conv_output_shape([1, 25, 40, 1], {\"size\":(4, 5), \"stride\": (2, 2), \"padding\":\"SAME\"}))\n",
    "print(conv_output_shape([1, 25, 40, 1], {\"size\":(4, 5), \"stride\": (3, 3), \"padding\":\"VALID\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Node connection functions\n",
    "\n",
    "def apply_matrix(input_node, train, parameters, options):\n",
    "    application = tf.matmul(input_node, parameters[0])\n",
    "    if len(parameters) > 1:\n",
    "        return application + parameters[1]\n",
    "    return application\n",
    "\n",
    "def apply_relu(input_node, train, parameters, options):\n",
    "    return tf.nn.relu(input_node)\n",
    "\n",
    "def apply_dropout(input_node, train, parameters, options):\n",
    "    if train:\n",
    "        return tf.nn.dropout(input_node, options[\"dropout_rate\"], seed=options[\"seed\"])\n",
    "    else:\n",
    "        return input_node\n",
    "\n",
    "def apply_conv(input_node, train, parameters, options):\n",
    "    stride = options[\"stride\"]\n",
    "    output = tf.nn.conv2d(input_node, parameters[0], [1, stride[0], stride[1], 1], padding=options[\"padding\"])\n",
    "    \n",
    "    if options[\"bias\"]:\n",
    "        output = output + parameters[1]\n",
    "        \n",
    "    return output\n",
    "\n",
    "def apply_pool(input_node, train, parameters, options):\n",
    "    if options[\"pool_type\"] == \"max\":\n",
    "        pool_function = tf.nn.max_pool\n",
    "    else:\n",
    "        pool_function = tf.nn.avg_pool\n",
    "    stride = [1, options[\"stride\"][0], options[\"stride\"][1], 1]\n",
    "    size = [1, options[\"size\"][0], options[\"size\"][1], 1]\n",
    "    return pool_function(input_node, size, stride, padding=options[\"padding\"])\n",
    "\n",
    "def apply_flatten(input_node, train, parameters, options):\n",
    "    return tf.reshape(input_node, flatten_output_shape(input_node.get_shape(), options))\n",
    "\n",
    "def apply_unflatten(input_node, train, parameters, options):\n",
    "    return tf.reshape(input_node, unflatten_output_shape(input_node.get_shape(), options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shape_test(shape, options):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        input = tf.placeholder(tf.float32, shape=shape)\n",
    "        parameters = setup_matrix(options)\n",
    "        result = apply_conv(input, False, parameters, options)\n",
    "        print(result.get_shape())\n",
    "    \n",
    "default_init = lambda size: tf.truncated_normal(size, stddev=0.1)\n",
    "shape_test([1, 25, 40, 1], {\"size\":(4, 5, 1, 1), \"stride\": (2, 2), \"padding\":\"VALID\", \"bias\":False, \"init\":default_init})\n",
    "shape_test([1, 25, 40, 1], {\"size\":(4, 5, 1, 1), \"stride\": (2, 2), \"padding\":\"SAME\", \"bias\":False, \"init\":default_init})\n",
    "shape_test([1, 25, 40, 1], {\"size\":(4, 5, 1, 1), \"stride\": (3, 3), \"padding\":\"VALID\", \"bias\":False, \"init\":default_init})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Setup and keep track of graph parameters and nodes for a layer.\"\"\"\n",
    "    def __init__(self, options, parameter_setup, node_setup):\n",
    "        self.options = options\n",
    "        self.parameter_setup = parameter_setup\n",
    "        self.node_setup = node_setup\n",
    "        self.parameters = None\n",
    "        \n",
    "    def setup_parameters(self):\n",
    "        self.parameters = self.parameter_setup(self.options)\n",
    "        \n",
    "    def connect(self, input_node, train):\n",
    "        node = self.node_setup(input_node, train, self.parameters, self.options)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Layer setup functions.\n",
    "\n",
    "def create_matrix_layer(inputs, channels, init=lambda size: tf.truncated_normal(size, stddev=0.1)):\n",
    "    in_size = inputs if isinstance(inputs, tuple) else (inputs,)\n",
    "    size = in_size + (channels,)\n",
    "    options = {\n",
    "        \"size\": size,\n",
    "        \"bias\": True,\n",
    "        \"init\": lambda size: init(size),\n",
    "        \"bias_init\": lambda size: init((channels,))\n",
    "    }\n",
    "    return Layer(options, setup_matrix, apply_matrix)\n",
    "\n",
    "def create_relu_layer():\n",
    "    return Layer({}, no_parameters, apply_relu)\n",
    "\n",
    "def create_dropout_layer(rate, seed):\n",
    "    options = {\n",
    "        \"dropout_rate\": rate,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "    return Layer(options, no_parameters, apply_dropout)\n",
    "\n",
    "def create_conv_layer(patch_size, stride, in_channels, out_channels, bias=True, padding=\"SAME\"):\n",
    "    init = lambda size: tf.truncated_normal(size, stddev=0.1)\n",
    "    options = {\n",
    "        \"size\": patch_size + (in_channels, out_channels),\n",
    "        \"bias\": bias,\n",
    "        \"init\": init,\n",
    "        \"bias_init\": init,\n",
    "        \"stride\": stride,\n",
    "        \"padding\": padding\n",
    "    }\n",
    "    return Layer(options, setup_matrix, apply_conv)\n",
    "\n",
    "def create_pool_layer(strategy, patch_size, stride, channels, padding=\"SAME\"):\n",
    "    options = {\n",
    "        \"pool_type\": strategy,\n",
    "        \"size\": patch_size,\n",
    "        \"stride\": stride,\n",
    "        \"padding\": padding\n",
    "    }\n",
    "    return Layer(options, no_parameters, apply_pool)\n",
    "\n",
    "def create_flatten_layer():\n",
    "    options = {}\n",
    "    return Layer(options, no_parameters, apply_flatten)\n",
    "\n",
    "def create_unflatten_layer(size):\n",
    "    options = {\n",
    "        \"size\": size\n",
    "    }\n",
    "    return Layer(options, no_parameters, apply_unflatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvoLayer(object):\n",
    "    \"\"\"Set up an evolvable layer representation.\"\"\"\n",
    "    def __init__(self, options):\n",
    "        self.options = options\n",
    "        \n",
    "    def output_size(self, input_size):\n",
    "        return input_size\n",
    "        \n",
    "    def can_mutate(self):\n",
    "        return False\n",
    "    \n",
    "    def mutate(self):\n",
    "        return self\n",
    "    \n",
    "    def make_layer(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_two_layer(batch_size, patch_size, depth, hidden_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        train = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, channel_count))\n",
    "        labels= tf.placeholder(tf.float32, shape=(batch_size, label_count))\n",
    "        valid = tf.constant(data[\"valid\"])\n",
    "        test  = tf.constant(data[\"test\"])\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        layers = [\n",
    "            create_conv_layer((patch_size, patch_size), (stride, stride), 1, depth),\n",
    "            create_relu_layer(),\n",
    "            create_conv_layer((patch_size, patch_size), (stride, stride), depth, depth),\n",
    "            create_relu_layer(),\n",
    "            create_flatten_layer(),\n",
    "            create_matrix_layer(image_size * image_size * depth / pow(stride, 4), hidden_size),\n",
    "            create_relu_layer(),\n",
    "            create_matrix_layer(hidden_size, label_count)\n",
    "        ]\n",
    "        \n",
    "        for layer in layers:\n",
    "            layer.setup_parameters()\n",
    "        \n",
    "        def model(nodes, train):\n",
    "            for layer in layers:\n",
    "                nodes.append(layer.connect(nodes[-1], train))\n",
    "            return nodes[-1]\n",
    "\n",
    "        logits = model([train], True)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(0.05).minimize(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"valid\": tf.nn.softmax(model([valid], False)),\n",
    "            \"test\":  tf.nn.softmax(model([test], False))\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_2conv = convnet_two_layer(batch_size=16, patch_size=5, depth=16, hidden_size=64, data=datasets)\n",
    "\n",
    "run_graph(graph_2conv, datasets, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_steps = 200000\n",
    "\n",
    "graph_connive = convnet_optimize(\n",
    "    batch_size=16, patch_sizes=[5,14], strides=[2,7], depths=[16,128],\n",
    "    hidden_sizes=[128,64],\n",
    "    rate_alpha=0.02, decay_rate=0.9, decay_steps=optimal_steps/4,\n",
    "    beta_loss=0.0005,\n",
    "    dropout_rate=0.5,\n",
    "    base_seed=45645,\n",
    "    data=datasets)\n",
    "\n",
    "run_graph(graph_connive, datasets, optimal_steps, report_every=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
