{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import traceback\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import outputer\n",
    "import convnet\n",
    "import mutate\n",
    "import convevo\n",
    "import darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload (convnet)\n",
    "reload (mutate)\n",
    "reload (convevo)\n",
    "reload (darwin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = '../ud730/notMNIST_full.pickle'\n",
    "\n",
    "train_dataset = []\n",
    "train_labels = []\n",
    "test_dataset = []\n",
    "test_labels = []\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_data(inputs_train, labels_train, inputs_test, labels_test):\n",
    "    data = {\n",
    "        \"image_size\": 28,\n",
    "        \"label_count\": 10,\n",
    "        \"channel_count\": 1\n",
    "    }\n",
    "    data[\"total_image_size\"] = data[\"image_size\"] * data[\"image_size\"]\n",
    "\n",
    "    def setup_data(inputs, labels, name):\n",
    "        inputs = inputs.reshape((-1, data[\"image_size\"], data[\"image_size\"], data[\"channel_count\"])).astype(np.float32)\n",
    "        # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "        labels = (np.arange(data[\"label_count\"]) == labels[:,None]).astype(np.float32)\n",
    "        print(name + \" set\", inputs.shape, labels.shape)\n",
    "        return inputs, labels\n",
    "    data[\"train\"], data[\"train_labels\"] = setup_data(inputs_train, train_labels, \"Training\")\n",
    "    data[\"test\"], data[\"test_labels\"] = setup_data(inputs_test, labels_test, \"Test\")\n",
    "    return data\n",
    "\n",
    "full_data = setup_data(train_dataset, train_labels, test_dataset, test_labels)\n",
    "print(full_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del train_labels\n",
    "del test_dataset\n",
    "del test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_validate(data, train_count, validate_count, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def randomize(inputs, labels):\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_inputs = inputs[permutation,:,:,:]\n",
    "        shuffled_labels = labels[permutation,:]\n",
    "        return shuffled_inputs, shuffled_labels\n",
    "\n",
    "    train_inputs = data[\"train\"][:]\n",
    "    train_labels = data[\"train_labels\"][:]\n",
    "    cross_data = copy.copy(data)\n",
    "\n",
    "    train_inputs, train_labels = randomize(train_inputs, train_labels)\n",
    "    cross_data[\"train\"] = train_inputs[:train_count]\n",
    "    cross_data[\"train_labels\"] = train_labels[:train_count]\n",
    "\n",
    "    cross_data[\"valid\"] = train_inputs[train_count:train_count + validate_count]\n",
    "    cross_data[\"valid_labels\"] = train_labels[train_count:train_count + validate_count]\n",
    "    return cross_data\n",
    "cross_data = setup_validate(full_data, 1000, 100)\n",
    "print(cross_data[\"train_labels\"].shape)\n",
    "print(cross_data[\"train_labels\"][0])\n",
    "print(full_data[\"train_labels\"][0])\n",
    "print(cross_data[\"valid\"].shape)\n",
    "del cross_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def graph_input_shape(batch_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    return (batch_size, image_size, image_size, channel_count)\n",
    "\n",
    "def graph_output_shape(batch_size, data):\n",
    "    return (batch_size, data[\"label_count\"])\n",
    "\n",
    "def setup_graph(batch_size, data, stack):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        input_shape = graph_input_shape(batch_size, data)\n",
    "        output_shape = graph_output_shape(batch_size, data)\n",
    "        train = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        labels= tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify= tf.placeholder(tf.float32, shape=input_shape)\n",
    "\n",
    "        layers = stack.construct(input_shape, output_shape)\n",
    "        l2_loss = convnet.setup_layers(layers)\n",
    "        \n",
    "        logits        = convnet.connect_model(train,  layers, True)[-1]\n",
    "        verify_logits = convnet.connect_model(verify, layers, False)[-1]\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels)) + l2_loss\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": stack.construct_optimizer(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": tf.nn.softmax(verify_logits),\n",
    "            \"saver\": tf.train.Saver()\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_accuracy(session, graph_info, inputs, labels, batch_size):\n",
    "    total_accuracy = 0\n",
    "    batch_count = len(inputs) / batch_size\n",
    "    for b in xrange(batch_count):\n",
    "        batch_data = inputs[b * batch_size: (b + 1) * batch_size]\n",
    "        feed_dict = { graph_info[\"verify\"] : batch_data }\n",
    "        predictions = session.run([graph_info[\"verify_predictions\"]], feed_dict=feed_dict)[0]\n",
    "        total_accuracy += accuracy(predictions, labels[b * batch_size: (b + 1) * batch_size]) / float(batch_count)\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_graph(\n",
    "    graph_info,\n",
    "    data,\n",
    "    step_count,\n",
    "    report_every=50,\n",
    "    verbose=True,\n",
    "    accuracy_minimum=None,\n",
    "    progress=None,\n",
    "    tracker=None\n",
    "):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        \n",
    "        # Optionally restore graph parameters from disk.\n",
    "        convnet.restore_model(graph_info, session)\n",
    "        \n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        valid_accuracy = 0\n",
    "        try:\n",
    "            for step in xrange(step_count + 1):\n",
    "                # Update progress bar, if present\n",
    "                if progress:\n",
    "                    progress.value = step\n",
    "\n",
    "                # Pick an offset within the training data, which has been randomized.\n",
    "                # Note: we could use better randomization across epochs.\n",
    "                offset = (step * batch_size) % (data[\"train_labels\"].shape[0] - batch_size)\n",
    "                # Generate a minibatch.\n",
    "                batch_data = data[\"train\"][offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = data[\"train_labels\"][offset:(offset + batch_size), :]\n",
    "                \n",
    "                # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "                # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "                # and the value is the numpy array to feed to it.\n",
    "                targets = [graph_info[\"optimizer\"], graph_info[\"loss\"], graph_info[\"predictions\"]]\n",
    "                feed_dict = {graph_info[\"train\"] : batch_data, graph_info[\"labels\"] : batch_labels}\n",
    "                \n",
    "                _, loss, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "                \n",
    "                reporting = step % report_every == 0\n",
    "                if reporting or tracker:\n",
    "                    batch_score = (loss, accuracy(predictions, batch_labels))\n",
    "                    if tracker:\n",
    "                        tracker(batch_score)\n",
    "                    \n",
    "                if np.isnan(loss):\n",
    "                    print(\"Error computing loss\")\n",
    "                    return valid_accuracy\n",
    "                \n",
    "                if reporting:\n",
    "                    if verbose:\n",
    "                        print(\"Minibatch loss at step\", step, \":\", loss)\n",
    "                        print(\"Minibatch accuracy: %.1f%%\" % batch_score[1])\n",
    "                    valid_accuracy = batch_accuracy(\n",
    "                        session, graph_info, data[\"valid\"], data[\"valid_labels\"], batch_size\n",
    "                    )\n",
    "                    print(\"Validation accuracy: %.1f%%\" % valid_accuracy)\n",
    "                    if accuracy_minimum and step > 0 and valid_accuracy < accuracy_minimum:\n",
    "                        print(\"Early out.\")\n",
    "                        break\n",
    "            if verbose:\n",
    "                test_accuracy = batch_accuracy(\n",
    "                    session, graph_info, data[\"test\"], data[\"test_labels\"], batch_size\n",
    "                )\n",
    "                print(\"Test accuracy: %.1f%%\" % test_accuracy)\n",
    "            return valid_accuracy\n",
    "        finally:\n",
    "            # Optionally save out graph parameters to disk.\n",
    "            convnet.save_model(graph_info, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_path = outputer.setup_directory(\"temp\", \"notMNIST_results\")\n",
    "\n",
    "def save_results(timestamp, results):\n",
    "    with open(os.path.join(results_path, timestamp + \".csv\"), \"w\") as text_file:\n",
    "        text_file.write(\"Loss,Accuracy\\n\")\n",
    "        for score in results:\n",
    "            text_file.write((\",\".join(str(v) for v in score)) + \"\\n\")\n",
    "    print(\"Saved results:\", timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_eval(\n",
    "    batch_size=16,\n",
    "    eval_steps=100000,\n",
    "    valid_steps=5000\n",
    "):\n",
    "    # Set up to show a progress bar so you some mesure of time required. Updated in run_graph above.\n",
    "    progress_bar = ipywidgets.FloatProgress(min=0, max=eval_steps, description=\"Graph Steps:\")\n",
    "    display(progress_bar)\n",
    "    \n",
    "    # Set up to show current training results as well as a running average. updated in record_score below. \n",
    "    def setup_label(title):\n",
    "        return ipywidgets.FloatText(value=0, description=title, disabled=True)\n",
    "    current_display = [setup_label(title) for title in [\"Loss\", \"Accuracy\"]]\n",
    "    average_display = [setup_label(\" \") for _ in current_display]\n",
    "    display(ipywidgets.HBox([\n",
    "        ipywidgets.Box([ipywidgets.HTML(\"<div style=\"\"margin-left:90px\"\">Current</div>\")] + current_display),\n",
    "        ipywidgets.Box([ipywidgets.HTML(\"<div style=\"\"margin-left:90px\"\">Running Average</div>\")] + average_display)\n",
    "    ]))\n",
    "    \n",
    "    def evaluate(stack, entropy):   \n",
    "        data = setup_validate(full_data, eval_steps, valid_steps)\n",
    "\n",
    "        try:\n",
    "            evo_graph = setup_graph(batch_size, data, stack)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            lines = traceback.format_exception(exc_type, exc_value, exc_traceback)\n",
    "            print(lines[-1])\n",
    "            convevo.output_error(stack, lines, \"temp\", outputer.timestamp(\"ERR~\", \"txt\"))\n",
    "            return -10\n",
    "\n",
    "        timestamp = outputer.timestamp()\n",
    "        with open(os.path.join(results_path, timestamp + \".xml\"), \"w\") as text_file:\n",
    "            text_file.write(convevo.serialize(stack))\n",
    "\n",
    "        # Record and display the results\n",
    "        results = []\n",
    "        def record_score(score):\n",
    "            results.append(score)\n",
    "            for display, value in zip(current_display, score):\n",
    "                display.value = value\n",
    "            \n",
    "            resultCount = min(len(results), 100)\n",
    "            averages = [sum(x)/resultCount for x in zip(*results[-resultCount:])]\n",
    "            for display, value in zip(average_display, averages):\n",
    "                display.value = value\n",
    "                \n",
    "        convnet.setup_save_model(evo_graph, os.path.join(results_path, timestamp + \".ckpt\"))\n",
    "        \n",
    "        try:\n",
    "            return run_graph(\n",
    "                evo_graph,\n",
    "                data,\n",
    "                eval_steps,\n",
    "                report_every=eval_steps/4,\n",
    "                verbose=False,\n",
    "                accuracy_minimum=50,\n",
    "                progress=progress_bar,\n",
    "                tracker=record_score\n",
    "            )\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            lines = traceback.format_exception(exc_type, exc_value, exc_traceback)\n",
    "            print(lines[-1])\n",
    "            convevo.output_error(stack, lines, results_path, timestamp + \"~ERR.txt\")\n",
    "            return -1\n",
    "        finally:\n",
    "            save_results(timestamp, results)\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_stack(patch_size, stride, depth, hidden_size, label_count, init_scale, optimizer_name=None):\n",
    "    if optimizer_name:\n",
    "        optimizer = convevo.Optimizer(optimizer_name, 0.05)\n",
    "        optimizer.default_parameters()\n",
    "    else:\n",
    "        optimizer = None\n",
    "        \n",
    "    conv_layers = [\n",
    "        (\"conv_bias\", patch_size, stride, depth, \"SAME\", True),\n",
    "        (\"conv_bias\", patch_size, stride, depth, \"SAME\", True)\n",
    "    ]\n",
    "\n",
    "    return convevo.create_stack(conv_layers, [], True, [hidden_size, label_count], 0.0, init_scale, 0.0, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_optimizers():\n",
    "    for optimizer_name in [\"GradientDescent\", \"Adadelta\", \"Adagrad\", \"Momentum\", \"Adam\", \"RMSProp\"]:\n",
    "        # As of this writing \"Ftrl\" is not supported on the GPU\n",
    "        batch_size = 16\n",
    "        eval_steps = 10000\n",
    "        test_stack = create_stack(5, 2, 64, 128, 10, 0.1, optimizer_name)\n",
    "        test_data = setup_validate(full_data, eval_steps * batch_size, 500)\n",
    "        test_graph = setup_graph(batch_size, test_data, test_stack)\n",
    "        print (run_graph(test_graph, test_data, eval_steps, report_every=eval_steps/4, verbose=False))\n",
    "test_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with outputer.TeeOutput(os.path.join(results_path, outputer.timestamp(\"Eval\", \"txt\"))):\n",
    "    test_stack = create_stack(5, 2, 64, 128, 10, 0.1)\n",
    "    print(convevo.serialize(test_stack))\n",
    "    make_eval()(test_stack, random.Random(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutate_entropy = random.Random(42)\n",
    "scored_mutants = []\n",
    "breed_options = {\n",
    "    \"input_shape\": graph_input_shape(16, full_data),\n",
    "    \"output_shape\": graph_output_shape(16, full_data)\n",
    "}\n",
    "mutator = darwin.Darwin(convevo.serialize, lambda s,e: 0, convevo.breed)\n",
    "prototypes = [create_stack(5, 2, 64, 64, 10, 0.1)]\n",
    "for mutant in mutator.init_population(prototypes, 20, False, breed_options, mutate_entropy):\n",
    "    scored_mutants.append((mutant, 0.0))\n",
    "    \n",
    "convevo.output_results(scored_mutants, \"temp\", \"mutants.xml\", 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutate_entropy = random.Random(42)\n",
    "mutant_children = []\n",
    "for _ in range(20):\n",
    "    mutant_a = mutate_entropy.choice(scored_mutants)[0]\n",
    "    mutant_b = mutate_entropy.choice(scored_mutants)[0]\n",
    "    mutant_children.append((convevo.breed([mutant_a, mutant_b], breed_options, mutate_entropy), 0.0))\n",
    "    \n",
    "convevo.output_results(mutant_children, \"temp\", \"mutant_offspring.xml\", 42, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prototypes = [\n",
    "    create_stack(5, 2, 64, 128, 10, 0.10, \"GradientDescent\"),\n",
    "    create_stack(6, 2, 128, 64, 10, 0.05, \"Adadelta\"),\n",
    "    create_stack(4, 2, 64, 128, 10, 0.10, \"Adagrad\"),\n",
    "    create_stack(5, 1, 256,128, 10, 0.02, \"Adam\"),\n",
    "    create_stack(2, 2, 64, 128, 10, 0.20, \"RMSProp\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with outputer.TeeOutput(os.path.join(\"temp\", outputer.timestamp(\"notMNIST_Evolve_\", \"txt\"))):\n",
    "    mutate_seed = random.randint(1, 100000)\n",
    "    print(\"Mutate Seed:\", mutate_seed)\n",
    "    mutate_entropy = random.Random(mutate_seed)\n",
    "    eval_seed = random.randint(1, 100000)\n",
    "    print(\"Eval Seed:\", eval_seed)\n",
    "    eval_entropy = random.Random(eval_seed)\n",
    "\n",
    "    population_size = 20\n",
    "    generations = 10\n",
    "    \n",
    "    breed_options = {\n",
    "        \"input_shape\": graph_input_shape(16, full_data),\n",
    "        \"output_shape\": graph_output_shape(16, full_data)\n",
    "    }\n",
    "\n",
    "    for stack in prototypes:\n",
    "        stack.make_safe(breed_options[\"input_shape\"], breed_options[\"output_shape\"])\n",
    "\n",
    "    charles = darwin.Darwin(convevo.serialize, make_eval(), convevo.breed)\n",
    "    charles.init_population(prototypes, population_size, True, breed_options, mutate_entropy)\n",
    "\n",
    "    for g in range(generations):\n",
    "        print(\"Generation\", g)\n",
    "        results = charles.evaluate(eval_entropy)\n",
    "        convevo.output_results(results, \"temp\", outputer.timestamp() + \".xml\")\n",
    "        charles.repopulate(population_size, 0.25, 4, results, breed_options, mutate_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = charles.best()\n",
    "print(\"Best score:\", best[1])\n",
    "print(convevo.serialize(best[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = darwin.descending_score(charles.history.values())\n",
    "convevo.output_results(results, \"testing\", \"notminist_full_evolve_run.xml\", mutate_seed, eval_seed)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for root, dirs, files in os.walk('temp'):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        low_name = name.lower()\n",
    "        if low_name.startswith(\"err\"):\n",
    "            with open (path, \"r\") as error_file:\n",
    "                lines=error_file.readlines()\n",
    "                errors.append((path, lines[-1]))\n",
    "\n",
    "for path, error in sorted(errors, key=lambda e: e[0]):\n",
    "    print(path)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "failed = 0\n",
    "print (convevo.serialize(charles.population[failed]))\n",
    "make_eval()(charles.population[failed], eval_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_population,_,eval_seed = convevo.load_population(\"testing/error.xml\")\n",
    "print(len(error_population))\n",
    "\n",
    "error_charles = darwin.Darwin(error_population, convevo.serialize, eval_stack, convevo.breed)\n",
    "error_results = error_charles.evaluate(random.Random(eval_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
