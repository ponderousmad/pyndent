{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "notMINST Data Setup\n",
    "===================\n",
    "\n",
    "This notebook sets up the the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST.\n",
    "\n",
    "This notebook is derived from the [Udacity Tensorflow Course Assignment 1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import outputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNWGtZaXn-5j"
   },
   "source": [
    "Download the dataset of characters 'A' to 'J' rendered in various fonts as 28x28 images.\n",
    "\n",
    "There is training set of about 500k images and a test set of about 19000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 186058,
     "status": "ok",
     "timestamp": 1444485672507,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "EYRJ4ICW6-da",
    "outputId": "0d0f85df-155f-4a89-8e7e-ee32df36ec8d"
   },
   "outputs": [],
   "source": [
    "url = \"http://yaroslavvb.com/upload/notMNIST/\"\n",
    "data_path = outputer.setup_directory(\"notMNIST\")\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print(\"Found\", filename, \"with correct size.\")\n",
    "    else:\n",
    "        raise Exception(\"Error downloading\" + filename)\n",
    "    return filename\n",
    "\n",
    "train_filename = maybe_download(os.path.join(data_path, \"notMNIST_large.tar.gz\"), 247336696)\n",
    "test_filename = maybe_download(os.path.join(data_path, \"notMNIST_small.tar.gz\"), 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cC3p0oEyF8QT"
   },
   "source": [
    "Extract the dataset from the compressed .tar.gz file.\n",
    "This should give you a set of directories, labelled A through J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 186055,
     "status": "ok",
     "timestamp": 1444485672525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "H8CBE-WZ8nmj",
    "outputId": "ef6c790c-2513-4b09-962e-27c79390c762"
   },
   "outputs": [],
   "source": [
    "def extract(filename, root, class_count):\n",
    "    dir_name = os.path.splitext(os.path.splitext(os.path.basename(filename))[0])[0]  # remove .tar.gz\n",
    "    path = os.path.join(root, dir_name)\n",
    "    print(\"Extracting\", filename, \"to\", path)\n",
    "    tar = tarfile.open(filename)\n",
    "    tar.extractall(path=root)\n",
    "    tar.close()\n",
    "    data_folders = [os.path.join(path, d) for d in sorted(os.listdir(path))]\n",
    "    if len(data_folders) != class_count:\n",
    "        raise Exception(\"Expected %d folders, one per class. Found %d instead.\" % (class_count, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "train_folders = []\n",
    "test_folders = []\n",
    "\n",
    "for name in os.listdir(data_path):\n",
    "    path = os.path.join(data_path, name)\n",
    "    target = None\n",
    "    print(\"Checking\", path)\n",
    "    if path.endswith(\"_small\"):\n",
    "        target = test_folders\n",
    "    elif path.endswith(\"_large\"):\n",
    "        target = train_folders\n",
    "    if target is not None:\n",
    "        target.extend([os.path.join(path, name) for name in os.listdir(path)])\n",
    "        print(\"Found\", target)\n",
    "\n",
    "expected_classes = 10\n",
    "\n",
    "if len(train_folders) < expected_classes:\n",
    "    train_folders = extract(train_filename, data_path, expected_classes)\n",
    "\n",
    "if len(test_folders) < expected_classes:\n",
    "    test_folders = extract(test_filename, data_path, expected_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4riXK3IoHgx6"
   },
   "source": [
    "# Inspect Data\n",
    "\n",
    "Verify that the images contain rendered glyphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"notMNIST/notMNIST_small/A/MDEtMDEtMDAudHRm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"notMNIST/notMNIST_large/A/a2F6b28udHRm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"notMNIST/notMNIST_large/C/ZXVyb2Z1cmVuY2UgaXRhbGljLnR0Zg==.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This I is all white\n",
    "Image(filename=\"notMNIST/notMNIST_small/I/SVRDIEZyYW5rbGluIEdvdGhpYyBEZW1pLnBmYg==.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBdkjESPK8tw"
   },
   "source": [
    "Convert the data into an array of normalized grayscale floating point images, and an array of classification labels.\n",
    "\n",
    "Unreadable images are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59"
   },
   "outputs": [],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "# Captured list of bad files from previous run (to avoid noisy output)\n",
    "skip_list = [\n",
    "    \"notMNIST/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png\",\n",
    "    \"notMNIST/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png\",\n",
    "    \"notMNIST/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png\",\n",
    "    \"notMNIST/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png\",\n",
    "    \"notMNIST/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png\",\n",
    "    \"notMNIST/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png\",\n",
    "    \"notMNIST/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png\"\n",
    "]\n",
    "\n",
    "def load(data_folders, set_id, min_count, max_count):\n",
    "    # Create arrays large enough for maximum expected data.\n",
    "    dataset = np.ndarray(shape=(max_count, image_size, image_size), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(max_count), dtype=np.int32)\n",
    "    label_index = 0\n",
    "    image_index = 0\n",
    "    \n",
    "    solid_blacks = []\n",
    "    solid_whites = []\n",
    "    \n",
    "    for folder in sorted(data_folders):\n",
    "        print(folder)\n",
    "        for image in os.listdir(folder):\n",
    "            if image_index >= max_count:\n",
    "                raise Exception(\"More than %d images!\" % (max_count,))\n",
    "            image_file = os.path.join(folder, image)\n",
    "            if image_file in skip_list:\n",
    "                continue\n",
    "            try:\n",
    "                raw_data = ndimage.imread(image_file)\n",
    "                \n",
    "                # Keep track of images a that are solid white or solid black.\n",
    "                if np.all(raw_data == 0):\n",
    "                    solid_blacks.append(image_file)\n",
    "                if np.all(raw_data == int(pixel_depth)):\n",
    "                    solid_whites.append(image_file)\n",
    "                \n",
    "                # Convert to float and normalize.\n",
    "                image_data = (raw_data.astype(float) - pixel_depth / 2) / pixel_depth\n",
    "\n",
    "                if image_data.shape != (image_size, image_size):\n",
    "                    raise Exception(\"Unexpected image shape: %s\" % str(image_data.shape))\n",
    "\n",
    "                # Capture the image data and label.\n",
    "                dataset[image_index, :, :] = image_data\n",
    "                labels[image_index] = label_index\n",
    "                image_index += 1\n",
    "            except IOError as e:\n",
    "                skip_list.append(image_file)\n",
    "                print(\"Could not read:\", image_file, ':', e, \"skipping.\")\n",
    "        label_index += 1\n",
    "    image_count = image_index\n",
    "    # Trim down to just the used portion of the arrays.\n",
    "    dataset = dataset[0:image_count, :, :]\n",
    "    labels = labels[0:image_count]\n",
    "    if image_count < min_count:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "    print(\"Input data shape:\", dataset.shape)\n",
    "    print(\"Mean of all normalized pixels:\", np.mean(dataset))\n",
    "    print(\"Standard deviation of normalized pixels:\", np.std(dataset))\n",
    "    print('Labels shape:', labels.shape)\n",
    "    print(\"Found\", len(solid_whites), \"solid white images, and\", len(solid_blacks), \"solid black images.\")\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset, train_labels = load(train_folders, \"train\", 450000, 550000)\n",
    "test_dataset, test_labels = load(test_folders, 'test', 18000, 20000)\n",
    "\n",
    "skip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUdbskYE2d87"
   },
   "source": [
    "# Verify Proccessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exemplar = plt.imshow(train_dataset[0])\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exemplar = plt.imshow(train_dataset[373])\n",
    "train_labels[373]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exemplar = plt.imshow(test_dataset[18169])\n",
    "test_labels[18169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exemplar = plt.imshow(train_dataset[-9])\n",
    "train_labels[-9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPTCnjIcyuKN"
   },
   "source": [
    "# Compress and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST/full.pickle'\n",
    "\n",
    "try:\n",
    "    f = gzip.open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
