{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and Demonstration of convnet, convevo and mutate Modules\n",
    "============================================================\n",
    "This notebook shows how convnet can be used to simplify constructing tensorflow convnets, and how convevo can be used to further define convnets in a way that allows them to be mutated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import outputer\n",
    "import convnet\n",
    "import mutate\n",
    "import convevo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "## Note: Requires notmnist_setup notebook to be run first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (529114, 28, 28) (529114,)\n",
      "Test set (18724, 28, 28) (18724,)\n",
      "Training set (529114, 28, 28, 1) (529114, 10)\n",
      "Test set (18724, 28, 28, 1) (18724, 10)\n",
      "dict_keys(['test', 'channel_count', 'total_image_size', 'label_count', 'test_labels', 'train', 'image_size', 'train_labels'])\n"
     ]
    }
   ],
   "source": [
    "def setup_data(pickle_file):\n",
    "    data = {\n",
    "        \"image_size\": 28,\n",
    "        \"label_count\": 10,\n",
    "        \"channel_count\": 1\n",
    "    }\n",
    "    data[\"total_image_size\"] = data[\"image_size\"] * data[\"image_size\"]\n",
    "    \n",
    "    with gzip.open(pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        inputs_train = save['train_dataset']\n",
    "        labels_train = save['train_labels']\n",
    "        inputs_test = save['test_dataset']\n",
    "        labels_test = save['test_labels']\n",
    "        print('Training set', inputs_train.shape, labels_train.shape)\n",
    "        print('Test set', inputs_test.shape, labels_test.shape)\n",
    "\n",
    "    def setup_data(inputs, labels, name):\n",
    "        shape = (-1, data[\"image_size\"], data[\"image_size\"], data[\"channel_count\"])\n",
    "        inputs = inputs.reshape(shape).astype(np.float32)\n",
    "        # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "        labels = (np.arange(data[\"label_count\"]) == labels[:,None]).astype(np.float32)\n",
    "        print(name + \" set\", inputs.shape, labels.shape)\n",
    "        return inputs, labels\n",
    "    data[\"train\"], data[\"train_labels\"]=setup_data(inputs_train, labels_train, \"Training\")\n",
    "    data[\"test\"], data[\"test_labels\"] = setup_data(inputs_test, labels_test, \"Test\")\n",
    "    return data\n",
    "\n",
    "full_data = setup_data('notMNIST/full.pickle')\n",
    "print(full_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training/validation split from the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_validate(data, train_count, validate_count, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def randomize(inputs, labels):\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_inputs = inputs[permutation,:,:,:]\n",
    "        shuffled_labels = labels[permutation,:]\n",
    "        return shuffled_inputs, shuffled_labels\n",
    "\n",
    "    train_inputs = data[\"train\"][:]\n",
    "    train_labels = data[\"train_labels\"][:]\n",
    "    cross_data = copy.copy(data)\n",
    "\n",
    "    train_inputs, train_labels = randomize(train_inputs, train_labels)\n",
    "    cross_data[\"train\"] = train_inputs[:train_count]\n",
    "    cross_data[\"train_labels\"] = train_labels[:train_count]\n",
    "\n",
    "    cross_data[\"valid\"] = train_inputs[train_count:train_count + validate_count]\n",
    "    cross_data[\"valid_labels\"] = train_labels[train_count:train_count + validate_count]\n",
    "    return cross_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 10)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "datasets = setup_validate(full_data, 200000, 10000)\n",
    "\n",
    "print(datasets[\"train_labels\"].shape)\n",
    "print(datasets[\"train_labels\"][0])\n",
    "print(full_data[\"train_labels\"][0])\n",
    "print(datasets[\"valid\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    correct_predictions = np.argmax(predictions, 1) == np.argmax(labels, 1)\n",
    "    return (100.0 * np.sum(correct_predictions) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_accuracy(session, graph_info, inputs, labels, batch_size):\n",
    "    total_accuracy = 0\n",
    "    batch_count = len(inputs) // batch_size\n",
    "    for b in range(batch_count):\n",
    "        batch_data = inputs[b * batch_size: (b + 1) * batch_size]\n",
    "        feed_dict = {graph_info[\"verify\"] : batch_data}\n",
    "        predictions = session.run([graph_info[\"verify_predictions\"]],\n",
    "                                  feed_dict=feed_dict)[0]\n",
    "        total_accuracy += accuracy(\n",
    "            predictions,\n",
    "            labels[b * batch_size: (b + 1) * batch_size]\n",
    "        )\n",
    "    return total_accuracy / float(batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_graph(\n",
    "    graph_info,\n",
    "    data,\n",
    "    step_count,\n",
    "    report_every=50,\n",
    "    verbose=True,\n",
    "    accuracy_minimum=None\n",
    "):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        valid_accuracy = 0\n",
    "        for step in range(step_count + 1):\n",
    "            # Generate a minibatch.\n",
    "            offset = (step * batch_size) % (data[\"train_labels\"].shape[0] - batch_size)\n",
    "            batch_data = data[\"train\"][offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = data[\"train_labels\"][offset:(offset + batch_size), :]\n",
    "            \n",
    "            # Set up graph inputs and targets\n",
    "            feed_dict = {\n",
    "                graph_info[\"train\"] : batch_data,\n",
    "                graph_info[\"labels\"] : batch_labels\n",
    "            }\n",
    "            targets = [\n",
    "                graph_info[\"optimizer\"],\n",
    "                graph_info[\"loss\"],\n",
    "                graph_info[\"predictions\"]\n",
    "            ]\n",
    "            _, loss, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "            \n",
    "            if np.isnan(loss):\n",
    "                print(\"Error computing loss\")\n",
    "                return 0\n",
    "            if (step % report_every == 0):\n",
    "                if verbose:\n",
    "                    print(\"Minibatch loss at step\", step, \":\", loss)\n",
    "                    print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions,\n",
    "                                                                  batch_labels))\n",
    "                valid_accuracy = batch_accuracy(\n",
    "                    session, graph_info, data[\"valid\"], data[\"valid_labels\"], batch_size\n",
    "                )\n",
    "                print(\"Validation accuracy: %.1f%%\" % valid_accuracy)\n",
    "                if accuracy_minimum and step > 0 and valid_accuracy < accuracy_minimum:\n",
    "                    print(\"Early out.\")\n",
    "                    break\n",
    "        if verbose:\n",
    "            test_accuracy = batch_accuracy(\n",
    "                session, graph_info, data[\"test\"], data[\"test_labels\"], batch_size\n",
    "            )\n",
    "            print(\"Test accuracy: %.1f%%\" % test_accuracy)\n",
    "        return valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests the shape calculations for convolution and pooling operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All shapes match.\n"
     ]
    }
   ],
   "source": [
    "def shape_test(shape, options, func):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        input = tf.placeholder(tf.float32, shape=shape)\n",
    "        parameters = convnet.setup_matrix(options)\n",
    "        result = func(input, False, parameters, options)\n",
    "        return tuple(int(d) for d in result.get_shape())\n",
    "    \n",
    "default_init = convnet.setup_initializer()\n",
    "correct = 0\n",
    "# For images of up to 7x7\n",
    "for w in range(1, 7):\n",
    "    # And patch sizes up to the image size\n",
    "    for p in range(1, w + 1):\n",
    "        # And strides up to the patch size\n",
    "        for s in range(1, p + 1):\n",
    "            # And for both same and valid padding\n",
    "            for pad in [\"SAME\", \"VALID\"]:\n",
    "                # And for both convolutions and pooling\n",
    "                for func in [convnet.apply_pool, convnet.apply_conv]:\n",
    "                    options = {\n",
    "                        \"size\":(p, p, 1, 1),\n",
    "                        \"stride\": (s, s),\n",
    "                        \"padding\":pad,\n",
    "                        \"pool_type\": \"max\",\n",
    "                        \"bias\":False,\n",
    "                        \"init\":default_init}\n",
    "                    # Check if calculated shape matches what Tensorflow actually does.\n",
    "                    calc = convnet.image_output_shape([1, w, w, 1], options)\n",
    "                    shape = shape_test([1, w, w, 1], options, func)\n",
    "                    if calc == shape:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        print(\"mismatch for\", w, p, s, pad, shape, calc)\n",
    "if correct == 224:\n",
    "    print(\"All shapes match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for a graph using convnet directly\n",
    "Graph is a simple two convolution layers with relu, then flatten followed by a hidden layer, relu then output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_two_layer(batch_size, patch_size, depth, hidden_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    total_image_size = image_size * image_size * depth\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Define the placeholders\n",
    "        input_shape = (batch_size, image_size, image_size, channel_count)\n",
    "        output_shape = (batch_size, label_count)\n",
    "        train = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        labels= tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify= tf.placeholder(tf.float32, shape=input_shape)\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        # Define the layers\n",
    "        operations = [\n",
    "            convnet.create_conv((patch_size, patch_size), (stride, stride), 1, depth),\n",
    "            convnet.create_relu(),\n",
    "            convnet.create_conv((patch_size, patch_size), (stride, stride), depth, depth),\n",
    "            convnet.create_relu(),\n",
    "            convnet.create_flatten(),\n",
    "            convnet.create_matrix(total_image_size // pow(stride, 4), hidden_size),\n",
    "            convnet.create_relu(),\n",
    "            convnet.create_matrix(hidden_size, label_count)\n",
    "        ]\n",
    "        \n",
    "        # Set up the graph variables for matrices and biases.\n",
    "        for op in operations:\n",
    "            op.setup_parameters()\n",
    "        \n",
    "        # Construct the graph nodes\n",
    "        def model(input, train):\n",
    "            nodes = [input]\n",
    "            for op in operations:\n",
    "                nodes.append(op.connect(nodes[-1], train))\n",
    "            return nodes[-1]\n",
    "\n",
    "        # Build the model for training.\n",
    "        logits = model(train, True)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        # Pass out all the neccesary bits.\n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(0.05).minimize(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": tf.nn.softmax(model(verify, False))\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 2.44979\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.2%\n",
      "Minibatch loss at step 100 : 1.2123\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 200 : 0.915699\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 300 : 0.601131\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 400 : 0.969889\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 500 : 0.570695\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 600 : 0.461254\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 700 : 1.00383\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 800 : 0.546082\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 900 : 0.190906\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 1000 : 0.618016\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.6%\n",
      "Test accuracy: 90.3%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.560000000000002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_2conv = convnet_two_layer(batch_size=16, patch_size=5,\n",
    "                                depth=16, hidden_size=64, data=datasets)\n",
    "\n",
    "run_graph(graph_2conv, datasets, 1000, 100, True, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for graph using convevo and mutate\n",
    "Graph is same as above: two convolution layers with relu, then flatten followed by and hidden layer, relu then output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_two_layer_stack(batch_size, patch_size, depth, hidden_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Define the placeholders\n",
    "        input_shape = (batch_size, image_size, image_size, channel_count)\n",
    "        output_shape = (batch_size, label_count)\n",
    "        train = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        labels= tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify= tf.placeholder(tf.float32, shape=input_shape)\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        # Define the layer stack\n",
    "        stack = convevo.LayerStack(flatten=True)\n",
    "        init = lambda: convevo.Initializer(\"normal\", scale=1)\n",
    "        \n",
    "        def add(layer, relu):\n",
    "            stack.add_layer(layer, relu=relu)\n",
    "        \n",
    "        add(convevo.ImageLayer(\"conv_bias\",patch_size, stride, depth, \"SAME\", init()),True)\n",
    "        add(convevo.ImageLayer(\"conv_bias\",patch_size, stride, depth, \"SAME\", init()),True)\n",
    "        add(convevo.HiddenLayer(hidden_size, bias=True, initializer=init()), True)\n",
    "        add(convevo.HiddenLayer(label_count, bias=True, initializer=init()), False)\n",
    "        \n",
    "        # Clone the stack and mutate it.\n",
    "        evo_copy = copy.deepcopy(stack)\n",
    "        evo_copy.mutate(input_shape, output_shape, None, random.Random(55))\n",
    "        \n",
    "        # Reset the seeds to ensure consistent initialization\n",
    "        evo_copy.reseed(random.Random(101))\n",
    "        \n",
    "        # Convert the stack to convnet style operations.\n",
    "        operations = evo_copy.construct(input_shape)\n",
    "        \n",
    "        # Setup the parameters for the operations (matrix and bias variables)\n",
    "        convnet.setup(operations)\n",
    "\n",
    "        # Construct the graph operations - the last one is the output.\n",
    "        logits = convnet.connect_model(train, operations, True)[-1]\n",
    "        verify_logits = convnet.connect_model(verify, operations, False)[-1]\n",
    "        \n",
    "        # Setup loss\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(0.05).minimize(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": tf.nn.softmax(verify_logits)\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 2.32898\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.6%\n",
      "Minibatch loss at step 50 : 1.51666\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 55.5%\n",
      "Minibatch loss at step 100 : 1.16974\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 150 : 1.03057\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 200 : 0.962429\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 250 : 0.700694\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 300 : 0.622969\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 350 : 0.795481\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 400 : 0.885985\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 450 : 0.334447\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 500 : 0.468145\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 550 : 0.940139\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 600 : 0.527906\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 650 : 0.721136\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 700 : 1.26754\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 750 : 0.500304\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 800 : 0.474956\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 850 : 0.20388\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 900 : 0.150118\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 950 : 0.556124\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 1000 : 0.611481\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 88.8%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.799999999999997"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "graph_2conv_stack = convnet_two_layer_stack(batch_size=16, patch_size=5,\n",
    "                                            depth=16, hidden_size=64, data=datasets)\n",
    "\n",
    "run_graph(graph_2conv, datasets, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Optimizer Using convevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_optimize(\n",
    "    batch_size,\n",
    "    patch_size,\n",
    "    depth,\n",
    "    hidden_size,\n",
    "    data,\n",
    "    rate_alpha=0.05,\n",
    "    decay_rate=1.0,\n",
    "    decay_steps=1000\n",
    "):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        input_shape = (batch_size, image_size, image_size, channel_count)\n",
    "        output_shape = (batch_size, label_count)\n",
    "        train = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        labels= tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify= tf.placeholder(tf.float32, shape=input_shape)\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        # Set up LayerStack\n",
    "        optimizer = convevo.Optimizer(\"GradientDescent\",rate_alpha,decay_rate,decay_steps)\n",
    "        stack = convevo.LayerStack(flatten=True, optimizer=optimizer)\n",
    "        init = lambda: convevo.Initializer(\"normal\", scale=0.1)\n",
    "\n",
    "        def add(layer, relu):\n",
    "            stack.add_layer(layer, relu=relu)\n",
    "        \n",
    "        add(convevo.ImageLayer(\"conv_bias\",patch_size, stride, depth, \"SAME\", init()),True)\n",
    "        add(convevo.ImageLayer(\"conv_bias\",patch_size, stride, depth, \"SAME\", init()),True)\n",
    "        add(convevo.HiddenLayer(hidden_size, bias=True, initializer=init()), True)\n",
    "        add(convevo.HiddenLayer(label_count, bias=True, initializer=init()), False)\n",
    "\n",
    "        operations = stack.construct(input_shape)\n",
    "        l2_loss = convnet.setup(operations)\n",
    "\n",
    "        logits = convnet.connect_model(train, operations, True)[-1]\n",
    "        verify_logits = convnet.connect_model(verify, operations, False)[-1]\n",
    "        loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,labels))+l2_loss\n",
    "       \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": stack.construct_optimizer(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": tf.nn.softmax(verify_logits)\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3.23063\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.5%\n",
      "Minibatch loss at step 10000 : 0.0329743\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 20000 : 0.295979\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 30000 : 0.586623\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 40000 : 0.145257\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 50000 : 0.710837\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 60000 : 0.308152\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 70000 : 0.532527\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 80000 : 0.0911913\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 90000 : 0.0588651\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 100000 : 0.327164\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Test accuracy: 96.1%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.640000000000001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_steps = 100000\n",
    "# convnet_optimize doesn't reset the seeds on the stack,\n",
    "# so initializers all default to using the global seed.\n",
    "tf.set_random_seed(45654)\n",
    "\n",
    "graph_connive = convnet_optimize(\n",
    "    batch_size=16, patch_size=5, depth=64, hidden_size=128,\n",
    "    rate_alpha=0.02, decay_rate=0.9, decay_steps=optimal_steps//4,\n",
    "    data=datasets\n",
    ")\n",
    "\n",
    "run_graph(graph_connive, datasets, optimal_steps, report_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
