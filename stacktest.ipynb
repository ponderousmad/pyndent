{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and Demostration of convnet and convevo and mutate Modules\n",
    "===============================================================\n",
    "This notebook shows how convnet can be used to simplify constructing tensorflow convnets, and how convevo can be used to further define convnets in a way that allows them to be mutated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import outputer\n",
    "import convnet\n",
    "import mutate\n",
    "import convevo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "## Note: Requires notmnist_setup notebook to be run first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_data(pickle_file):\n",
    "    data = {\n",
    "        \"image_size\": 28,\n",
    "        \"label_count\": 10,\n",
    "        \"channel_count\": 1\n",
    "    }\n",
    "    data[\"total_image_size\"] = data[\"image_size\"] * data[\"image_size\"]\n",
    "    \n",
    "    with gzip.open(pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        inputs_train = save['train_dataset']\n",
    "        labels_train = save['train_labels']\n",
    "        inputs_test = save['test_dataset']\n",
    "        labels_test = save['test_labels']\n",
    "        print('Training set', inputs_train.shape, labels_train.shape)\n",
    "        print('Test set', inputs_test.shape, labels_test.shape)\n",
    "\n",
    "    def setup_data(inputs, labels, name):\n",
    "        shape = (-1, data[\"image_size\"], data[\"image_size\"], data[\"channel_count\"])\n",
    "        inputs = inputs.reshape(shape).astype(np.float32)\n",
    "        # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "        labels = (np.arange(data[\"label_count\"]) == labels[:,None]).astype(np.float32)\n",
    "        print(name + \" set\", inputs.shape, labels.shape)\n",
    "        return inputs, labels\n",
    "    data[\"train\"], data[\"train_labels\"] = setup_data(inputs_train, labels_train, \"Training\")\n",
    "    data[\"test\"], data[\"test_labels\"] = setup_data(inputs_test, labels_test, \"Test\")\n",
    "    return data\n",
    "\n",
    "full_data = setup_data('notMNIST/full.pickle')\n",
    "print(full_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training/validation split from the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_validate(data, train_count, validate_count, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def randomize(inputs, labels):\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_inputs = inputs[permutation,:,:,:]\n",
    "        shuffled_labels = labels[permutation,:]\n",
    "        return shuffled_inputs, shuffled_labels\n",
    "\n",
    "    train_inputs = data[\"train\"][:]\n",
    "    train_labels = data[\"train_labels\"][:]\n",
    "    cross_data = copy.copy(data)\n",
    "\n",
    "    train_inputs, train_labels = randomize(train_inputs, train_labels)\n",
    "    cross_data[\"train\"] = train_inputs[:train_count]\n",
    "    cross_data[\"train_labels\"] = train_labels[:train_count]\n",
    "\n",
    "    cross_data[\"valid\"] = train_inputs[train_count:train_count + validate_count]\n",
    "    cross_data[\"valid_labels\"] = train_labels[train_count:train_count + validate_count]\n",
    "    return cross_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = setup_validate(full_data, 200000, 10000)\n",
    "\n",
    "print(datasets[\"train_labels\"].shape)\n",
    "print(datasets[\"train_labels\"][0])\n",
    "print(full_data[\"train_labels\"][0])\n",
    "print(datasets[\"valid\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_accuracy(session, graph_info, inputs, labels, batch_size):\n",
    "    total_accuracy = 0\n",
    "    batch_count = len(inputs) / batch_size\n",
    "    for b in xrange(batch_count):\n",
    "        batch_data = inputs[b * batch_size: (b + 1) * batch_size]\n",
    "        feed_dict = {graph_info[\"verify\"] : batch_data}\n",
    "        predictions = session.run([graph_info[\"verify_predictions\"]], feed_dict=feed_dict)[0]\n",
    "        total_accuracy += accuracy(predictions, labels[b * batch_size: (b + 1) * batch_size]) / float(batch_count)\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_graph(graph_info, data, step_count, report_every=50, verbose=True, accuracy_minimum=None):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        valid_accuracy = 0\n",
    "        for step in xrange(step_count + 1):\n",
    "            # Generate a minibatch.\n",
    "            offset = (step * batch_size) % (data[\"train_labels\"].shape[0] - batch_size)\n",
    "            batch_data = data[\"train\"][offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = data[\"train_labels\"][offset:(offset + batch_size), :]\n",
    "            \n",
    "            # Set up graph inputs and targets\n",
    "            feed_dict = {graph_info[\"train\"] : batch_data, graph_info[\"labels\"] : batch_labels}\n",
    "            targets = [graph_info[\"optimizer\"], graph_info[\"loss\"], graph_info[\"predictions\"]]\n",
    "            _, loss, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "            \n",
    "            if np.isnan(loss):\n",
    "                print(\"Error computing loss\")\n",
    "                return 0\n",
    "            if (step % report_every == 0):\n",
    "                if verbose:\n",
    "                    print(\"Minibatch loss at step\", step, \":\", loss)\n",
    "                    print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                valid_accuracy = batch_accuracy(\n",
    "                    session, graph_info, data[\"valid\"], data[\"valid_labels\"], batch_size\n",
    "                )\n",
    "                print(\"Validation accuracy: %.1f%%\" % valid_accuracy)\n",
    "                if accuracy_minimum and step > 0 and valid_accuracy < accuracy_minimum:\n",
    "                    print(\"Early out.\")\n",
    "                    break\n",
    "        if verbose:\n",
    "            test_accuracy = batch_accuracy(session, graph_info, data[\"test\"], data[\"test_labels\"], batch_size)\n",
    "            print(\"Test accuracy: %.1f%%\" % test_accuracy)\n",
    "        return valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests the shape calculations for convolution and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shape_test(shape, options, func):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        input = tf.placeholder(tf.float32, shape=shape)\n",
    "        parameters = convnet.setup_matrix(options)\n",
    "        result = func(input, False, parameters, options)\n",
    "        return tuple(int(d) for d in result.get_shape())\n",
    "    \n",
    "default_init = convnet.setup_initializer()\n",
    "correct = 0\n",
    "# For images of up to 7x7\n",
    "for w in xrange(1, 7):\n",
    "    # And patch sizes up to the image size\n",
    "    for p in xrange(1, w + 1):\n",
    "        # And strides up to the patch size\n",
    "        for s in xrange(1, p + 1):\n",
    "            # And for both same and valid padding\n",
    "            for pad in [\"SAME\", \"VALID\"]:\n",
    "                # And for both convolutions and pooling\n",
    "                for func in [convnet.apply_pool, convnet.apply_conv]:\n",
    "                    options = {\n",
    "                        \"size\":(p, p, 1, 1),\n",
    "                        \"stride\": (s, s),\n",
    "                        \"padding\":pad,\n",
    "                        \"pool_type\": \"max\",\n",
    "                        \"bias\":False,\n",
    "                        \"init\":default_init}\n",
    "                    # Check that the calculated shape matches what Tensorflow actually does.\n",
    "                    calc = convnet.image_output_shape([1, w, w, 1], options)\n",
    "                    shape = shape_test([1, w, w, 1], options, func)\n",
    "                    if calc == shape:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        print(\"mismatch for\", w, p, s, pad, shape, calc)\n",
    "if correct == 224:\n",
    "    print(\"All shapes match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for a graph using convnet directly\n",
    "Graph is a simple two convolution layers with relu, then flatten followed by and hidden layer, relu then output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_two_layer(batch_size, patch_size, depth, hidden_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Define the placeholders\n",
    "        input_shape = (batch_size, image_size, image_size, channel_count)\n",
    "        output_shape = (batch_size, label_count)\n",
    "        train = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        labels= tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify= tf.placeholder(tf.float32, shape=input_shape)\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        # Define the layers\n",
    "        layers = [\n",
    "            convnet.create_conv_layer((patch_size, patch_size), (stride, stride), 1, depth),\n",
    "            convnet.create_relu_layer(),\n",
    "            convnet.create_conv_layer((patch_size, patch_size), (stride, stride), depth, depth),\n",
    "            convnet.create_relu_layer(),\n",
    "            convnet.create_flatten_layer(),\n",
    "            convnet.create_matrix_layer(image_size * image_size * depth / pow(stride, 4), hidden_size),\n",
    "            convnet.create_relu_layer(),\n",
    "            convnet.create_matrix_layer(hidden_size, label_count)\n",
    "        ]\n",
    "        \n",
    "        # Set up the graph variables for matrices and biases.\n",
    "        for layer in layers:\n",
    "            layer.setup_parameters()\n",
    "        \n",
    "        # Construct the graph nodes\n",
    "        def model(input, train):\n",
    "            nodes = [input]\n",
    "            for layer in layers:\n",
    "                nodes.append(layer.connect(nodes[-1], train))\n",
    "            return nodes[-1]\n",
    "\n",
    "        # Build the model for training.\n",
    "        logits = model(train, True)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        # Pass out all the neccesary bits.\n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(0.05).minimize(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": tf.nn.softmax(model(verify, False))\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_2conv = convnet_two_layer(batch_size=16, patch_size=5, depth=16, hidden_size=64, data=datasets)\n",
    "\n",
    "run_graph(graph_2conv, datasets, 1000, 100, True, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for graph using convevo and mutate\n",
    "Graph is same as above: two convolution layers with relu, then flatten followed by and hidden layer, relu then output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_two_layer_stack(batch_size, patch_size, depth, hidden_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Define the placeholders\n",
    "        input_shape = (batch_size, image_size, image_size, channel_count)\n",
    "        output_shape = (batch_size, label_count)\n",
    "        train = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        labels= tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify= tf.placeholder(tf.float32, shape=input_shape)\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        # Define the layer stack\n",
    "        evo_stack = convevo.LayerStack(flatten=True)\n",
    "        init = lambda: convevo.Initializer(\"normal\", scale=1)\n",
    "        \n",
    "        evo_stack.add_layer(convevo.ImageLayer(\"conv_bias\", patch_size, stride, depth, \"SAME\", init()), relu=True)\n",
    "        evo_stack.add_layer(convevo.ImageLayer(\"conv_bias\", patch_size, stride, depth, \"SAME\", init()), relu=True)\n",
    "        evo_stack.add_layer(convevo.HiddenLayer(hidden_size, bias=True, initializer=init()), relu=True)\n",
    "        evo_stack.add_layer(convevo.HiddenLayer(label_count, bias=True, initializer=init()), relu=False)\n",
    "        \n",
    "        # Clone the stack and mutate it.\n",
    "        evo_copy = copy.deepcopy(evo_stack)\n",
    "        evo_copy.mutate(input_shape, output_shape, random.Random(55))\n",
    "        \n",
    "        # Reset the seeds to ensure consistent initialization\n",
    "        evo_copy.reseed(random.Random(101))\n",
    "        \n",
    "        # Convert the stack to convnet style layers.\n",
    "        layers = evo_copy.construct(input_shape)\n",
    "        \n",
    "        # Setup the parameters for the layers (matrix and bias variables)\n",
    "        convnet.setup_layers(layers)\n",
    "\n",
    "        # Construct the graph operations - the last one is the output.\n",
    "        logits = convnet.connect_model(train, layers, True)[-1]\n",
    "        verify_logits = convnet.connect_model(verify, layers, False)[-1]\n",
    "        \n",
    "        # Setup loss\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(0.05).minimize(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": tf.nn.softmax(verify_logits)\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "graph_2conv_stack = convnet_two_layer_stack(batch_size=16, patch_size=5, depth=16, hidden_size=64, data=datasets)\n",
    "\n",
    "run_graph(graph_2conv, datasets, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Optimizer Using convevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convnet_optimize(\n",
    "    batch_size,\n",
    "    patch_size,\n",
    "    depth,\n",
    "    hidden_size,\n",
    "    data,\n",
    "    rate_alpha=0.05,\n",
    "    decay_rate=1.0,\n",
    "    decay_steps=1000\n",
    "):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        input_shape = (batch_size, image_size, image_size, channel_count)\n",
    "        output_shape = (batch_size, label_count)\n",
    "        train = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        labels= tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify= tf.placeholder(tf.float32, shape=input_shape)\n",
    "        \n",
    "        stride = 2\n",
    "        \n",
    "        evo_optimize = convevo.Optimizer(\"GradientDescent\", rate_alpha, decay_rate, decay_steps)\n",
    "        \n",
    "        evo_stack = convevo.LayerStack(flatten=True, optimizer=evo_optimize)\n",
    "        init = lambda: convevo.Initializer(\"normal\", scale=0.1)\n",
    "        \n",
    "        evo_stack.add_layer(convevo.ImageLayer(\"conv_bias\", patch_size, stride, depth, \"SAME\", init()), relu=True)\n",
    "        evo_stack.add_layer(convevo.ImageLayer(\"conv_bias\", patch_size, stride, depth, \"SAME\", init()), relu=True)\n",
    "        evo_stack.add_layer(convevo.HiddenLayer(hidden_size, bias=True, initializer=init()), relu=True)\n",
    "        evo_stack.add_layer(convevo.HiddenLayer(label_count, bias=True, initializer=init()), relu=False)\n",
    "\n",
    "        layers = evo_stack.construct(input_shape)\n",
    "        l2_loss = convnet.setup_layers(layers)\n",
    "\n",
    "        logits = convnet.connect_model(train, layers, True)[-1]\n",
    "        verify_logits = convnet.connect_model(verify, layers, False)[-1]\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels)) + l2_loss\n",
    "       \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": evo_stack.construct_optimizer(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": tf.nn.softmax(verify_logits)\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_steps = 200000\n",
    "# convnet_optimize doesn't reset the seeds on the stack, so initializers all default to using the global seed.\n",
    "tf.set_random_seed(45654)\n",
    "\n",
    "graph_connive = convnet_optimize(\n",
    "    batch_size=16, patch_size=5, depth=64, hidden_size=128,\n",
    "    rate_alpha=0.02, decay_rate=0.9, decay_steps=optimal_steps/4,\n",
    "    data=datasets\n",
    ")\n",
    "\n",
    "run_graph(graph_connive, datasets, optimal_steps, report_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
