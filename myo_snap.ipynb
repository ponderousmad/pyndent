{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Learning Snap\n",
    "=============\n",
    "\n",
    "Train an LSTM model to detect snapping fingers in the Myo armband data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "import outputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"../MyoSnap/testing\"\n",
    "data_files = [\n",
    "    \"nothing.csv\"\n",
    "]\n",
    "\n",
    "for i in range(11):\n",
    "    data_files.append(\"trial\" + str(i) + \".csv\")\n",
    "\n",
    "data_sets = {}\n",
    "\n",
    "snap_threshold = 0.001\n",
    "\n",
    "for file_name in data_files:\n",
    "    with open(os.path.join(data_path, file_name), 'r') as f:\n",
    "        skip_count = 20 # Give signals time to stabalize\n",
    "        start_time = None\n",
    "        emg_data = []\n",
    "        snap_times = []\n",
    "        was_quiet = True\n",
    "        snap_count = 0\n",
    "        for line in f:\n",
    "            if skip_count > 0:\n",
    "                skip_count -= 1\n",
    "            else:\n",
    "                parts = line.strip().split(\",\")\n",
    "                if parts[0] == \"E\":\n",
    "                    emg_data.append((int(parts[1]), [int(v) for v in parts[2:]]))\n",
    "                elif parts[0] == \"A\":\n",
    "                    energy = float(parts[2])\n",
    "                    if was_quiet and energy > snap_threshold:\n",
    "                        snap_times.append(int(parts[1]))\n",
    "                        snap_count += 1\n",
    "                    was_quiet = energy < snap_threshold\n",
    "        name = file_name[:-4]\n",
    "        data_sets[name] = (emg_data, snap_times)\n",
    "        print(name, \"samples:\", len(emg_data), \"snaps:\", snap_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def npEMG(data_set):\n",
    "    emg_data = data_set[0]\n",
    "    data = np.zeros([len(emg_data), len(emg_data[0][1]) + 1], dtype=np.float)\n",
    "    for i, entry in enumerate(emg_data):\n",
    "        data[i][0] = entry[0]\n",
    "        data[i][1:] = entry[1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emg2 = npEMG(data_sets[\"trial2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emg2_norm = emg2[:,1:]/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.min(emg2_norm, axis=0))\n",
    "print(np.max(emg2_norm, axis=0))\n",
    "print(np.average(emg2_norm, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8)\n",
    "pca.fit(emg2_norm)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit an ICA model to the data\n",
    "ica = FastICA(random_state=42)\n",
    "ica.fit(emg2_norm)\n",
    "\n",
    "print(ica.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, emg, snaps, snap_duration, size, unrolls):\n",
    "        self._emg = emg\n",
    "        self._snaps = snaps\n",
    "        self._snap_duration = snap_duration\n",
    "        self._channels = len(emg[0][1])\n",
    "        self._batch_size = size\n",
    "        self._unrolls = unrolls\n",
    "        segment = len(self._emg) // size\n",
    "        self._cursor = [ offset * segment for offset in range(size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, self._channels), dtype=np.float)\n",
    "        label = np.zeros(shape=(self._batch_size, 1), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            entry = self._emg[self._cursor[b]]\n",
    "            batch[b, :] = entry[1]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % len(self._emg)\n",
    "            for snap in self._snaps:\n",
    "                offset = entry[0] - snap\n",
    "                if offset > 0 and offset < self._snap_duration:\n",
    "                    label[b][0] = 1.0\n",
    "        return (batch / 128.0, label)\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by unrolls new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._unrolls):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batcher_test = BatchGenerator(data_sets[\"trial0\"][0], data_sets[\"trial0\"][1], 100000, 10, 5)\n",
    "print(batcher_test.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_graph(node_count, channel_count, label_count, batch_size, unrolls):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        gate_count = 4\n",
    "        # Parameters:\n",
    "        # Gates: input, previous output, and bias.\n",
    "        input_weights = tf.Variable(tf.truncated_normal([channel_count, node_count * gate_count], -0.1, 0.1))\n",
    "        output_weights = tf.Variable(tf.truncated_normal([node_count, node_count * gate_count], -0.1, 0.1))\n",
    "        bias = tf.Variable(tf.zeros([1, node_count * gate_count]))\n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, node_count]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, node_count]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([node_count, label_count], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([label_count]))\n",
    "\n",
    "        # Definition of the cell computation.\n",
    "        def lstm_cell(i, o, state):\n",
    "            \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "            Note that in this formulation, we omit the various connections between the\n",
    "            previous state and the gates.\"\"\"\n",
    "            values = tf.matmul(i, input_weights) + tf.matmul(o, output_weights) + bias\n",
    "            values = tf.split(1, gate_count, values)\n",
    "            input_gate = tf.sigmoid(values[0])\n",
    "            forget_gate = tf.sigmoid(values[1])\n",
    "            update = values[2]\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "            output_gate = tf.sigmoid(values[3])\n",
    "            return output_gate * tf.tanh(state), state\n",
    "\n",
    "        # Input data.\n",
    "        train_inputs = list()\n",
    "        train_labels = list()\n",
    "        for _ in range(unrolls):\n",
    "            train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, channel_count]))\n",
    "            train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, label_count]))\n",
    "\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs:\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # State saving across unrollings.\n",
    "        with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "            # Classifier.\n",
    "            logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    return {\n",
    "        \"graph\": graph,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"unrolls\": unrolls,\n",
    "        \"train_inputs\": train_inputs,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"loss\": loss,\n",
    "        \"train_prediction\": train_prediction,\n",
    "        \"learning_rate\": learning_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_lstm(setup, training, validation, snap_duration, step_count, report_every):\n",
    "    train_batches = BatchGenerator(\n",
    "        training[0], training[1],\n",
    "        snap_duration,\n",
    "        setup[\"batch_size\"], setup[\"unrolls\"]\n",
    "    )\n",
    "    # valid_batches = batcher(validation, 1, 1)\n",
    "    with tf.Session(graph=setup[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(step_count + 1):\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = {}\n",
    "            for i in range(setup[\"unrolls\"]):\n",
    "                feed_dict[setup[\"train_inputs\"][i]] = batches[i][0]\n",
    "                feed_dict[setup[\"train_labels\"][i]] = batches[i][1]\n",
    "                \n",
    "            targets = [\n",
    "                setup[\"optimizer\"],\n",
    "                setup[\"loss\"],\n",
    "                setup[\"train_prediction\"],\n",
    "                setup[\"learning_rate\"]\n",
    "            ]\n",
    "\n",
    "            _, l, predictions, lr = session.run(targets, feed_dict=feed_dict)\n",
    "\n",
    "            mean_loss += l\n",
    "            if step % report_every == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / report_every\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate([b[1] for b in batches])\n",
    "                print(predictions.shape)\n",
    "                print(labels)\n",
    "                \n",
    "                # Measure validation set perplexity.\n",
    "                #valid_logprob = 0\n",
    "                #for _ in range(len(validation)):\n",
    "                #    b = valid_batches.next()\n",
    "                #    predictions = setup[\"sample_prediction\"].eval({setup[\"sample_input\"]: b[0]})\n",
    "                #    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                #print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setup = setup_graph(20, len(data_sets[\"trial0\"][0][0][1]), 1, 128, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_lstm(setup, data_sets[\"trial0\"], data_sets[\"trial10\"], 100000, 1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
