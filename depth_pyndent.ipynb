{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evolving Convnets to Reconstruct Depth Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import gc\n",
    "import ipywidgets\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from scipy import ndimage\n",
    "from scipy.misc import imsave\n",
    "\n",
    "import outputer\n",
    "import improc\n",
    "import convnet\n",
    "import mutate\n",
    "import convevo\n",
    "import darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reload (improc)\n",
    "reload (convnet)\n",
    "reload (mutate)\n",
    "reload (convevo)\n",
    "reload (darwin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Enumerate Images\n",
    "Image names are sequential, so add every tenth image to the validation set based on filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training, test = improc.enumerate_images(\"captures\")\n",
    "\n",
    "print(\"Training:\", len(training), \"Test:\", len(test))\n",
    "print(training[:2])\n",
    "print(test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Processing\n",
    "Each image file contains a color image (top half), and an encoded depth image (bottom half)\n",
    "<img src=\"testing/IMG_2114.PNG\">\n",
    "* Note: The image may also contain the orientation data. If so it is encoded in the first two pixels of the depth image. If the first pixel of the depth image is red, the second has the x, y, z, w quaternion components encoded in the r,g,b,a values.\n",
    "\n",
    "The improc module contains functions for splitting the image, decoding the depth back into floating point millimeters, and for filling in gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Precomputed via compute_average_depth()\n",
    "# Actually it should 1680.24, value below is actually the mean of the image means.\n",
    "# Keeping this value as it was what was used in the experiments to date,\n",
    "# and it is close to the correct value.\n",
    "MEAN_DEPTH = np.float32(1688.97)\n",
    "NORMALIZED_MEAN_DEPTH = MEAN_DEPTH / improc.MAX_DEPTH\n",
    "\n",
    "print(MEAN_DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "depth_image_cache_path = outputer.setup_directory(\"temp\", \"cache\")\n",
    "\n",
    "class ImageSampler(object):\n",
    "    \"\"\"Wrap an image for sampling.\"\"\"\n",
    "    def __init__(self, image_file, make_holes, zero_mean=False):\n",
    "        # Process the image or grab it from the cache.\n",
    "        # image is normalized CIELAB, depth is not normalized.\n",
    "        self.image, self.depth = improc.process_cached(depth_image_cache_path, image_file)\n",
    "        self.depth /= improc.MAX_DEPTH\n",
    "        self.make_holes = make_holes\n",
    "        self.mean_offset = 0\n",
    "        if zero_mean:\n",
    "            self.mean_offset = NORMALIZED_MEAN_DEPTH\n",
    "            self.depth -= self.mean_offset\n",
    "            \n",
    "    def scale_size(self, size, pixel_count):\n",
    "        return int(size * pixel_count / 100.0)\n",
    "        \n",
    "    def make_hole_mask(self, image_slot, entropy, bias=-0.15, noise_scales=None):\n",
    "        mask_height = image_slot.shape[0] \n",
    "        mask_width = image_slot.shape[1]\n",
    "        if not noise_scales:\n",
    "            noise_scales = [\n",
    "                (self.scale_size(2, mask_height), self.scale_size(2, mask_width), 0.6),\n",
    "                (self.scale_size(40, mask_height), self.scale_size(20, mask_width), .35),\n",
    "                (mask_height // 2, mask_width // 2, 0.05)\n",
    "            ]\n",
    "        noise_image = np.zeros(shape=(image_slot.shape[:2]))\n",
    "        for y_scale, x_scale, amplitude in noise_scales:\n",
    "            noise_image += improc.make_noise(\n",
    "                mask_height, mask_width, y_scale, x_scale, entropy\n",
    "            ) * amplitude\n",
    "        return noise_image, noise_image > bias\n",
    "\n",
    "    def sample(self, image_slot, depth_slot,\n",
    "               mask_slot=None, h_offset=None, w_offset=None, entropy=np.random):\n",
    "        height = image_slot.shape[0]\n",
    "        spare_height = self.image.shape[0] - height\n",
    "        y = spare_height // 2 if h_offset is None else h_offset\n",
    "        \n",
    "        width = image_slot.shape[1]\n",
    "        spare_width = self.image.shape[1] - width\n",
    "        x = spare_width // 2 if w_offset is None else w_offset\n",
    "        \n",
    "        if self.make_holes:\n",
    "            input_depth = image_slot[:,:,-1]\n",
    "            input_depth[:,:] = improc.mipmap_imputer(\n",
    "                self.depth[y : y + height, x : x + width], smooth=True\n",
    "            )\n",
    "            _, mask = self.make_hole_mask(input_depth, entropy)\n",
    "            input_depth[np.where(mask)] = -1\n",
    "            image_slot = image_slot[:,:,:-1]\n",
    "            if mask_slot is not None:\n",
    "                mask_slot[:,:,0] = mask\n",
    "        else:\n",
    "            image_slot = image_slot[:,:,:]\n",
    "        \n",
    "        image_slot[:,:,:] = self.image[y : y+height, x : x+width, : image_slot.shape[-1]]\n",
    "        depth_slot[:,:,0] = self.depth[y : y+height, x : x+width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_images(paths, inputs, targets, masks=None, entropy=np.random):\n",
    "    for i, sampler in enumerate([ImageSampler(path, masks is not None) for path in paths]):\n",
    "        sampler.sample(inputs[i], targets[i],\n",
    "                       None if masks is None else masks[i], entropy=entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Image processing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_image, example_depth, example_attitude = improc.load_image(\"testing/IMG_2114.PNG\")\n",
    "plt.imshow(example_image)\n",
    "print(example_image.shape, example_image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(example_depth)\n",
    "print(example_depth.shape, example_depth.dtype)\n",
    "print(example_attitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sampler = ImageSampler(\"testing/IMG_2114.PNG\", False)\n",
    "sample_size = 100\n",
    "image_sample = np.zeros(shape=(sample_size, sample_size, 3))\n",
    "depth_sample = np.zeros(shape=(sample_size, sample_size, 1))\n",
    "sampler.sample(image_sample, depth_sample)\n",
    "\n",
    "print(image_sample.shape, image_sample.dtype)\n",
    "plt.imshow(image_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(depth_sample.reshape(sample_size, sample_size))\n",
    "print(depth_sample.shape, depth_sample.dtype)\n",
    "print(np.min(depth_sample), np.max(depth_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sampler = ImageSampler(\"testing/IMG_2114.PNG\", True)\n",
    "sample_size = 100\n",
    "image_sample = np.zeros(shape=(sample_size, sample_size, 4))\n",
    "depth_sample = np.zeros(shape=(sample_size, sample_size, 1))\n",
    "sample_mask = np.zeros(shape=(sample_size, sample_size, 1))\n",
    "sampler.sample(image_sample, depth_sample, sample_mask, entropy=np.random.RandomState(11))\n",
    "\n",
    "print(image_sample.shape, image_sample.dtype)\n",
    "plt.imshow(image_sample[:,:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(sample_mask[:,:,0])\n",
    "print(np.sum(sample_mask) / np.sum(np.ones_like(sample_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "COLOR_CHANNELS = 3\n",
    "image_height = 480\n",
    "image_width = 640\n",
    "\n",
    "data_files = {\n",
    "    \"image_size\": (image_height, image_width, COLOR_CHANNELS),\n",
    "    \"depth_size\": (image_height, image_width, 1),\n",
    "    \"train_files\": np.array(sorted(training)),\n",
    "    \"test_files\": np.array(sorted(test))\n",
    "}\n",
    "\n",
    "del training\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def setup_cross_validation(\n",
    "    data, valid_count, test_count=None, chunk_size=None, entropy=random\n",
    "):\n",
    "    cross_data = data.copy()\n",
    "    \n",
    "    if chunk_size:\n",
    "        cross_data[\"image_size\"] = chunk_size\n",
    "        cross_data[\"depth_size\"] = chunk_size[:-1] + (1,)\n",
    "\n",
    "    paths = cross_data[\"train_files\"][:]\n",
    "    mutate.fisher_yates_shuffle(paths, entropy)\n",
    "\n",
    "    cross_data[\"train_files\"] = paths[:-valid_count]\n",
    "    cross_data[\"valid_files\"] = paths[-valid_count:]\n",
    "    \n",
    "    if test_count is None:\n",
    "        del cross_data[\"test_files\"]\n",
    "    else:\n",
    "        cross_data[\"test_files\"] = data[\"test_files\"][:test_count]\n",
    "    \n",
    "    return cross_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def setup_graph(\n",
    "    batch_size,\n",
    "    image_shape,\n",
    "    target_shape,\n",
    "    stack\n",
    "):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        input_shape = (batch_size,) + image_shape\n",
    "        output_shape = (batch_size,) + target_shape\n",
    "        train   = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        targets = tf.placeholder(tf.float32, shape=output_shape)\n",
    "        verify  = tf.placeholder(tf.float32, shape=input_shape)\n",
    "\n",
    "        operations = stack.construct(input_shape, output_shape)\n",
    "        l2_loss = convnet.setup(operations)\n",
    "        \n",
    "        results = convnet.connect_model(train, operations, True)[-1]\n",
    "        \n",
    "        # Fill NaNs in target with values from results to\n",
    "        # eliminate any contribution to the gradient\n",
    "        valid_targets = tf.where(tf.is_nan(targets), results, targets)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.squared_difference(results, valid_targets)) + l2_loss\n",
    "        \n",
    "        verify_predictions = convnet.connect_model(verify, operations, False)[-1]\n",
    "        verify_predictions = tf.maximum(verify_predictions, 0)\n",
    "        verify_predictions = tf.minimum(verify_predictions, 1)\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"targets\": targets,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": stack.construct_optimizer(loss),\n",
    "            \"predictions\": results,\n",
    "            \"verify\": verify,\n",
    "            \"verify_predictions\": verify_predictions,\n",
    "            \"saver\": tf.train.Saver()\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prediction_error(predictions, targets):\n",
    "    is_finite = np.isfinite(targets)\n",
    "    where_valid = np.where(is_finite)\n",
    "    error = np.mean(np.absolute(predictions[where_valid] - targets[where_valid]))\n",
    "    return error, np.count_nonzero(is_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_predictor(session, graph_info):\n",
    "    def predict(inputs, targets):\n",
    "        feed_dict = {graph_info[\"verify\"]: inputs}\n",
    "        return session.run([graph_info[\"verify_predictions\"]], feed_dict=feed_dict)[0]\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mask_targets(targets, masks):\n",
    "    if masks is not None:\n",
    "        targets[np.where(np.logical_not(masks))] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_prediction_error(\n",
    "    predictor, files, inputs, targets, masks, batch_size, entropy=np.random\n",
    "):\n",
    "    total_error = 0\n",
    "    total_count = 0\n",
    "    batch_count = len(files) // batch_size\n",
    "    for b in range(batch_count):\n",
    "        offset = b * batch_size\n",
    "        end = offset + batch_size\n",
    "        prepare_images(files[offset:end], inputs, targets, masks, entropy)\n",
    "        predictions = predictor(inputs, targets)\n",
    "        mask_targets(targets, masks)\n",
    "        error, count = prediction_error(predictions, targets)\n",
    "        total_error += error * count\n",
    "        total_count += count\n",
    "    return (total_error / np.float32(total_count)), predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def depth_mean_like(depths):\n",
    "    return np.ones_like(depths) * (MEAN_DEPTH / improc.MAX_DEPTH)\n",
    "\n",
    "def always_guess_mean_error(files, inputs, targets, masks, batch_size, entropy):\n",
    "    def predict_mean(images, depths):\n",
    "        return depth_mean_like(depths)\n",
    "    return batch_prediction_error(\n",
    "        predict_mean,\n",
    "        files, inputs,\n",
    "        targets, masks,\n",
    "        batch_size, entropy\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_input_shape(batch_size, data, make_holes):\n",
    "    shape = (batch_size,) + data[\"image_size\"]\n",
    "    if make_holes:\n",
    "        shape = shape[:-1] + (shape[-1] + 1,)\n",
    "    return shape\n",
    "\n",
    "def batch_output_shape(batch_size, data):\n",
    "    return (batch_size,) + data[\"depth_size\"]\n",
    "\n",
    "def score_run(guess_mean_error, valid_error):\n",
    "    return guess_mean_error - min(valid_error, 1)\n",
    "\n",
    "def run_graph(\n",
    "    graph_info,\n",
    "    data,\n",
    "    make_holes,\n",
    "    step_count,\n",
    "    report_every=50,\n",
    "    verbose=True,\n",
    "    tracker=None,\n",
    "    track_minibatch_error=False,\n",
    "    mean_error_cache=None,\n",
    "    error_maximum=None,\n",
    "    prepare_seeds=None\n",
    "):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "\n",
    "        # Optionally restore graph parameters from disk.\n",
    "        convnet.restore_model(graph_info, session)\n",
    "        \n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        batch_inputs = np.empty(shape=batch_input_shape(batch_size, data, make_holes),\n",
    "                                dtype=np.float32)\n",
    "        batch_targets = np.empty(shape=batch_output_shape(batch_size, data),\n",
    "                                 dtype=np.float32)\n",
    "        if make_holes:\n",
    "            batch_masks = np.empty(shape=batch_targets.shape, dtype=np.bool)\n",
    "        else:\n",
    "            batch_masks = None\n",
    "\n",
    "        # Validation and scoring bits.\n",
    "        valid_error = 1\n",
    "        if prepare_seeds is None:\n",
    "            prepare_seeds = [random.randint(1, 12345), random.randint(1, 12345)]\n",
    "        \n",
    "        guess_mean_error = None\n",
    "        if mean_error_cache is not None:\n",
    "            guess_mean_error = mean_error_cache.get(\"cached\")\n",
    "        if not guess_mean_error:\n",
    "            guess_mean_error = always_guess_mean_error(\n",
    "                data[\"valid_files\"], batch_inputs, batch_targets, batch_masks, batch_size,\n",
    "                np.random.RandomState(prepare_seeds[1])\n",
    "            )\n",
    "            print(\"Error if just guess mean:\", guess_mean_error)\n",
    "            \n",
    "            if mean_error_cache is not None:\n",
    "                mean_error_cache[\"cached\"] = guess_mean_error\n",
    "        predictor = make_predictor(session, graph_info)\n",
    "        prepare_entropy = np.random.RandomState(prepare_seeds[0])\n",
    "        \n",
    "        training_files = data[\"train_files\"]\n",
    "        try:\n",
    "            for step in range(step_count + 1):\n",
    "                if tracker:\n",
    "                    tracker.update_progress(step)\n",
    "\n",
    "                # Generate a minibatch.\n",
    "                offset = (step * batch_size) % (training_files.shape[0] - batch_size)\n",
    "                batch_files = training_files[offset:(offset + batch_size)]\n",
    "                prepare_images(\n",
    "                    batch_files, batch_inputs, batch_targets, batch_masks, prepare_entropy\n",
    "                )\n",
    "\n",
    "                # Graph evaluation targets:\n",
    "                targets = [\n",
    "                    graph_info[\"optimizer\"],\n",
    "                    graph_info[\"loss\"],\n",
    "                    graph_info[\"predictions\"]\n",
    "                ]\n",
    "                \n",
    "                # Graph inputs:\n",
    "                feed_dict = {\n",
    "                    graph_info[\"train\"] : batch_inputs,\n",
    "                    graph_info[\"targets\"] : batch_targets\n",
    "                }\n",
    "                \n",
    "                # Run the graph\n",
    "                _, loss, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "                \n",
    "                # Capture last prediction\n",
    "                results = (predictions[-1], batch_targets[-1], batch_inputs[-1])\n",
    "                \n",
    "                # Update stats:\n",
    "                reporting = step % report_every == 0\n",
    "                if reporting or track_minibatch_error:\n",
    "                    batch_error, _ = prediction_error(predictions, batch_targets)\n",
    "                else:\n",
    "                    batch_error = None\n",
    "                if tracker:\n",
    "                    tracker.record_score((loss, batch_error))\n",
    "                \n",
    "                if not np.isfinite(loss):\n",
    "                    print(\"Error computing loss:\", loss)\n",
    "                    print(np.sum(np.isnan(predictions)))\n",
    "                    return score_run(guess_mean_error, valid_error), results\n",
    "                \n",
    "                if reporting:\n",
    "                    if verbose:\n",
    "                        print(\"Minibatch loss at step\", step, \":\", loss)\n",
    "                        print(\"Minibatch error:\", batch_error)\n",
    "                    valid_error, _, _ = batch_prediction_error(\n",
    "                        predictor, data[\"valid_files\"],\n",
    "                        batch_inputs, batch_targets, batch_masks, batch_size,\n",
    "                        np.random.RandomState(prepare_seeds[1])\n",
    "                    )\n",
    "                    print(\"Validation error:\", valid_error)\n",
    "                    if error_maximum and step > 0 and valid_error > error_maximum:\n",
    "                        print(\"Early out.\")\n",
    "                        break\n",
    "            test_files = data.get(\"test_files\")\n",
    "            if test_files is not None:\n",
    "                test_results = batch_prediction_error(\n",
    "                    predictor, test_files,\n",
    "                    batch_inputs, batch_targets, batch_masks, batch_size,\n",
    "                    np.random.RandomState(prepare_seeds[1])\n",
    "                )\n",
    "                print(\"Test error:\", test_results[0])\n",
    "                results = results + test_results\n",
    "            return score_run(guess_mean_error, valid_error), results\n",
    "        finally:\n",
    "            # Optionally save out graph parameters to disk.\n",
    "            convnet.save_model(graph_info, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TEST_BATCH = 1\n",
    "test_inputs = np.empty(shape=(TEST_BATCH, 480, 640, COLOR_CHANNELS + 1), dtype=np.float32)\n",
    "test_depths = np.empty_like(test_inputs[:,:,:,:1])\n",
    "test_mask = np.empty_like(test_depths)\n",
    "prepare_images(data_files[\"test_files\"][:TEST_BATCH], test_inputs, test_depths, test_mask,\n",
    "               np.random.RandomState(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_inputs[0,:,:,0], cmap='Greys_r')\n",
    "print(np.min(test_inputs[0,:,:,0]),np.max(test_inputs[0,:,:,0]))\n",
    "print(np.min(test_inputs[0,:,:,1]),np.max(test_inputs[0,:,:,1]))\n",
    "print(np.min(test_inputs[0,:,:,2]),np.max(test_inputs[0,:,:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_depths[0,:,:,0])\n",
    "depths_valid = np.where(np.isfinite(test_depths[0]))\n",
    "print(np.min(test_depths[0][depths_valid]),np.max(test_depths[0][depths_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(test_inputs.shape)\n",
    "print(test_depths.shape)\n",
    "prediction_error(test_inputs[:,:,:,0:1], test_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction_error(np.zeros_like(test_depths), test_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction_error(depth_mean_like(test_depths), test_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction_error(test_depths, test_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mask_targets(test_depths, test_mask)\n",
    "prediction_error(depth_mean_like(test_depths), test_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_depths[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TEST_BATCH = 1\n",
    "MAKE_HOLES = True\n",
    "conv_layers = [\n",
    "    (\"conv\",       5, 2, 10, \"SAME\", False),\n",
    "    (\"conv\",      10, 2, 20, \"SAME\", False),\n",
    "    (\"conv_bias\", 15, 5, 25, \"SAME\", False)\n",
    "]\n",
    "expand_layers = [\n",
    "    (5, 5, \"SAME\", True, False),\n",
    "    (2, 5, \"SAME\", True, False),\n",
    "    (2, 5, \"SAME\", True, False)\n",
    "]\n",
    "test_input_shape = batch_input_shape(TEST_BATCH, data_files, MAKE_HOLES)\n",
    "test_output_shape = batch_output_shape(TEST_BATCH, data_files)\n",
    "test_stack = convevo.create_stack(conv_layers, expand_layers, False, [], 0.0, 0.01, 0.0)\n",
    "test_stack.make_safe(test_input_shape, test_output_shape)\n",
    "test_stack.reseed(random.Random(24601))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_size = data_files[\"image_size\"]\n",
    "depth_size = data_files[\"depth_size\"]\n",
    "\n",
    "test_graph = setup_graph(TEST_BATCH, test_input_shape[1:], depth_size, test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_cross = setup_cross_validation(data_files, 200, 200, sample_size)\n",
    "test_score, test_results = run_graph(\n",
    "    test_graph, test_cross, MAKE_HOLES, 8, 4, True, prepare_seeds=[654, 321]\n",
    ")\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_results[0][:,:,0])\n",
    "print(np.min(test_results[0]),np.max(test_results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_results[1].reshape(sample_size[0],sample_size[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_results[2][:,:,0], cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del test_stack\n",
    "del test_graph\n",
    "del test_cross\n",
    "del test_results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evolving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_path = outputer.setup_directory(\"temp\", \"pyndent_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_eval(batch_size, eval_steps, valid_size, reuse_cross, make_holes, entropy=random):\n",
    "    mean_error_cache = None\n",
    "    if reuse_cross:\n",
    "        redata = setup_cross_validation(\n",
    "            data_files, valid_size, entropy=entropy\n",
    "        )\n",
    "        mean_error_cache = {}\n",
    "\n",
    "    progress_tracker = outputer.ProgressTracker(\n",
    "        [\"Loss\", \"Error\"], eval_steps, results_path, convevo.serialize\n",
    "    )\n",
    "    \n",
    "    prepare_seeds = [random.randint(1, 12345), random.randint(1, 12345)]\n",
    "\n",
    "    def evaluate(stack, eval_entropy):\n",
    "        # If not reusing data, generate training and validation sets\n",
    "        if not reuse_cross:\n",
    "            data = setup_cross_validation(\n",
    "                data_files, valid_size, entropy=eval_entropy\n",
    "            )\n",
    "        else:\n",
    "            data = redata\n",
    "       \n",
    "        progress_tracker.setup_eval(stack)\n",
    "\n",
    "        # Set up the graph\n",
    "        try:\n",
    "            graph_info = setup_graph(\n",
    "                batch_size,\n",
    "                batch_input_shape(batch_size, data, make_holes)[1:],\n",
    "                data[\"depth_size\"],\n",
    "                stack\n",
    "            )\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            progress_tracker.error(sys.exc_info())\n",
    "            return -10\n",
    "\n",
    "        progress_tracker.start_eval(graph_info)\n",
    "                \n",
    "        # Run the graph\n",
    "        try:\n",
    "            valid_error, _ = run_graph(\n",
    "                graph_info,\n",
    "                data,\n",
    "                make_holes,\n",
    "                eval_steps,\n",
    "                report_every=eval_steps//10,\n",
    "                verbose=True,\n",
    "                tracker=progress_tracker,\n",
    "                mean_error_cache=mean_error_cache,\n",
    "                error_maximum=None,\n",
    "                prepare_seeds=prepare_seeds\n",
    "            )\n",
    "            return valid_error\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            progress_tracker.error(sys.exc_info())\n",
    "            return -1\n",
    "        finally:\n",
    "            progress_tracker.output()\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Specific Evolutionary Experiments\n",
    "Only run one of the following. In the case of the hole filling, also set **make_holes** to True in the **Evolve!** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Basic convolutional network.\n",
    "conv_layers = [\n",
    "    (\"conv\",       5, 2, 10, \"SAME\", False),\n",
    "    (\"conv\",      10, 2, 20, \"SAME\", False),\n",
    "    (\"conv_bias\", 15, 5, 25, \"SAME\", False)\n",
    "]\n",
    "expand_layers = [\n",
    "    (5, 5, \"SAME\", True, False),\n",
    "    (2, 5, \"SAME\", True, False),\n",
    "    (2, 5, \"SAME\", True, False)\n",
    "]\n",
    "prototype = convevo.create_stack(conv_layers, expand_layers, False, [], 0.0, 0.01, 0.0)\n",
    "\n",
    "prototypes = [prototype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Past evolutionary experiment\n",
    "population,_,_ = convevo.load_population(\"testing/pyndent_evolve_run.xml\", False)\n",
    "prototypes = population[:5]\n",
    "print(len(prototypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hand selected/tuned protototypes\n",
    "prototypes = [\n",
    "    convevo.load_stack(\"testing/pyndent1.xml\"),\n",
    "    convevo.load_stack(\"testing/pyndent2.xml\"),\n",
    "    convevo.load_stack(\"testing/pyndent3.xml\"),\n",
    "    convevo.load_stack(\"testing/pyndent4.xml\"),\n",
    "    convevo.load_stack(\"testing/pyndent5.xml\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evoloved prototype\n",
    "prototypes = [\n",
    "    convevo.load_stack(\"testing/pyndent6/2016-06-22~17_51_16_872.xml\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hole filling variant experiments\n",
    "prototypes = [\n",
    "    convevo.load_stack(\"testing/passthrough.xml\"),\n",
    "    convevo.load_stack(\"testing/holes1.xml\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evolve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with outputer.TeeOutput(os.path.join(\"temp\", outputer.timestamp(\"Pyndent_evo\", \"txt\"))):\n",
    "    mutate_seed = random.randint(1, 100000)\n",
    "    print(\"Mutate Seed:\", mutate_seed)\n",
    "    mutate_entropy = random.Random(mutate_seed)\n",
    "    eval_seed = random.randint(1, 100000)\n",
    "    print(\"Eval Seed:\", eval_seed)\n",
    "    eval_entropy = random.Random(eval_seed)\n",
    "\n",
    "    population_size = 10\n",
    "    generations = 5\n",
    "    batch_size = 1\n",
    "    make_holes = False\n",
    "\n",
    "    breed_options = {\n",
    "        \"input_shape\": batch_input_shape(batch_size, data_files, make_holes),\n",
    "        \"output_shape\": batch_output_shape(batch_size, data_files),\n",
    "        \"fixed_stride\": make_holes,\n",
    "        \"fixed_padding\": make_holes\n",
    "    }\n",
    "\n",
    "    # Ensure loaded networks match input/output shapes for this run\n",
    "    for stack in prototypes:\n",
    "        stack.make_safe(breed_options[\"input_shape\"], breed_options[\"output_shape\"])\n",
    "\n",
    "    evaluator = make_eval(\n",
    "        batch_size=batch_size, eval_steps=80000, valid_size=400,\n",
    "        reuse_cross=True, make_holes=make_holes, entropy=eval_entropy\n",
    "    )\n",
    "    charles = darwin.Darwin(convevo.serialize, evaluator, convevo.breed)\n",
    "    charles.init_population(\n",
    "        prototypes, population_size, True, breed_options, mutate_entropy\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for g in range(generations):\n",
    "            print(\"Generation\", g)\n",
    "            results = charles.evaluate(eval_entropy)\n",
    "            convevo.output_results(\n",
    "                results, \"temp\", outputer.timestamp() + \".xml\", mutate_seed, eval_seed\n",
    "            )\n",
    "            charles.repopulate(\n",
    "                population_size, 0.3, 3, results, breed_options, mutate_entropy\n",
    "            )\n",
    "    finally:\n",
    "        results = darwin.descending_score(charles.history.values())\n",
    "        convevo.output_results(results, \"temp\", \"pyndent_evo.xml\", mutate_seed,eval_seed)\n",
    "        print(\"Evaluated\", len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Candidate Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_test_images(graph_info, data, make_holes, output_path, entropy):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "\n",
    "        # restore graph parameters from disk.\n",
    "        convnet.restore_model(graph_info, session)\n",
    "        \n",
    "        # Set up space for graph inputs\n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        batch_inputs = np.empty(shape=batch_input_shape(batch_size, data, make_holes),\n",
    "                                dtype=np.float32)\n",
    "        batch_targets = np.empty(shape=batch_output_shape(batch_size, data),\n",
    "                                 dtype=np.float32)\n",
    "        batch_masks = None\n",
    "        if make_holes:\n",
    "            batch_masks = np.empty(shape=batch_targets.shape, dtype=np.bool)\n",
    "\n",
    "        predictor = make_predictor(session, graph_info)\n",
    "\n",
    "        # Set up progress bar\n",
    "        test_files = data[\"test_files\"]\n",
    "        eval_count = len(test_files) // batch_size\n",
    "        progress = outputer.show_progress(\"Evaluation Steps:\", eval_count)\n",
    "        \n",
    "        # Only score error on same portion of the image as classy so we can compare.\n",
    "        input_image_size = batch_inputs.shape[1:]\n",
    "        classy_sample = (101, 101)\n",
    "        sample_start = tuple(cs // 2 for cs in classy_sample)\n",
    "        sample_end = tuple(\n",
    "            ss+iis-cs for ss,iis,cs in zip(sample_start, input_image_size, classy_sample)\n",
    "        )\n",
    "        classy_depth_start = (input_image_size[0] + sample_start[0], sample_start[1])\n",
    "        classy_depth_end = tuple(\n",
    "             cds+se-ss for cds,se,ss in zip(classy_depth_start, sample_end, sample_start)\n",
    "        )\n",
    "        def sample_patch(image, index):\n",
    "            return image[\n",
    "                index,\n",
    "                sample_start[0] : sample_end[0],\n",
    "                sample_start[1] : sample_end[1],\n",
    "                :\n",
    "            ]\n",
    "        \n",
    "        titles = [\"Name\", \"Error\", \"Count\"]\n",
    "        print(\",\".join(titles))\n",
    "        \n",
    "        all_scores = []\n",
    "        error_sum = 0\n",
    "        pixel_count = 0\n",
    "        for step in range(eval_count):            \n",
    "            progress.value = step # Update progress bar\n",
    "            \n",
    "            # Load the test data and calculate predicted depths.\n",
    "            offset = step * batch_size\n",
    "            end = offset + batch_size\n",
    "            batch_files = test_files[offset:end]\n",
    "            prepare_images(batch_files, batch_inputs, batch_targets, batch_masks)\n",
    "            predictions = predictor(batch_inputs, batch_targets)\n",
    "            mask_targets(batch_targets, batch_masks)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                image_path = batch_files[i]\n",
    "                image_name, ext = os.path.splitext(os.path.basename(image_path))\n",
    "\n",
    "                # Calculate metrics\n",
    "                error, count = prediction_error(\n",
    "                    sample_patch(predictions, i), sample_patch(batch_targets, i)\n",
    "                )\n",
    "                results = [image_name, error, count]\n",
    "                error_sum += error * count\n",
    "                pixel_count += count\n",
    "                all_scores.append(results)\n",
    "        \n",
    "                # Encode result images\n",
    "                image = ndimage.imread(image_path)\n",
    "                encoded_depths = improc.encode_normalized_depths(predictions)\n",
    "\n",
    "                # Classy patch\n",
    "                image[\n",
    "                    classy_depth_start[0] : classy_depth_end[0],\n",
    "                    classy_depth_start[1] : classy_depth_end[1],\n",
    "                    :\n",
    "                ] = sample_patch(encoded_depths, i)\n",
    "                imsave(os.path.join(output_path, image_name + \"_framed.png\"), image)\n",
    "                \n",
    "                # Full image\n",
    "                image[input_image_size[0]:,:,:] = encoded_depths[i]\n",
    "                imsave(os.path.join(output_path, image_name + \"_full.png\"), image)\n",
    "                \n",
    "                print(\",\".join(str(v) for v in results))\n",
    "\n",
    "        # Aggregate results\n",
    "        print(\",\".join([\"Total\", str(error_sum / pixel_count), str(pixel_count)]))\n",
    "              \n",
    "        sorted_scores = sorted(all_scores, key=lambda l: l[1])\n",
    "        print(titles[1] + \" high\")\n",
    "        print(\",\".join([str(v) for v in sorted_scores[-1]]))\n",
    "        print(titles[1] + \" low\")\n",
    "        print(\",\".join([str(v) for v in sorted_scores[0]]))\n",
    "              \n",
    "        return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results_path = outputer.setup_directory(\"temp/pyndent6\")\n",
    "BATCH_SIZE = 1\n",
    "MAKE_HOLES = False\n",
    "\n",
    "with outputer.TeeOutput(os.path.join(test_results_path, \"test_results.txt\")):\n",
    "    candidate = convevo.load_stack(\n",
    "        \"testing/pyndent6/2016-07-15~20_30_58_640.xml\"\n",
    "    )\n",
    "    test_data = setup_cross_validation(\n",
    "        data_files, 0, 1123, entropy=random.Random(121)\n",
    "    )\n",
    "    test_input_shape = batch_input_shape(BATCH_SIZE, test_data, MAKE_HOLES)\n",
    "    candidate_graph = setup_graph(\n",
    "        BATCH_SIZE, test_input_shape[1:], test_data[\"depth_size\"], candidate\n",
    "    )\n",
    "    convnet.setup_restore_model(\n",
    "        candidate_graph, candidate.checkpoint_path()\n",
    "    )\n",
    "    candidate_test_scores = compute_test_images(\n",
    "        candidate_graph, test_data, MAKE_HOLES, test_results_path, random.Random(57)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
